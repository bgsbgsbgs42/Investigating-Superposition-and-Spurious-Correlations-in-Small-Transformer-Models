{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "zoAcEHu-99vg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_multifeature_data(n_samples=1000, n_features=5, feature_dim=10):\n",
        "    \"\"\"Generate synthetic data with multiple independent features\"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        # Generate independent feature vectors\n",
        "        sample_features = []\n",
        "        for _ in range(n_features):\n",
        "            feature = np.random.randn(feature_dim)\n",
        "            sample_features.append(feature)\n",
        "\n",
        "        # Create label as nonlinear combination of features\n",
        "        label = np.sum([np.sum(f**2) for f in sample_features]) > n_features * feature_dim/2\n",
        "\n",
        "        features.append(np.concatenate(sample_features))\n",
        "        labels.append(label)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "def generate_biased_data(n_samples=1000, n_features=5, feature_dim=10, bias_strength=0.8):\n",
        "    \"\"\"Generate dataset with intentional spurious correlations\"\"\"\n",
        "    features, labels = generate_multifeature_data(n_samples, n_features, feature_dim)\n",
        "\n",
        "    # Introduce spurious correlation\n",
        "    bias_feature = np.random.randn(n_samples, feature_dim)\n",
        "    bias_labels = (np.sum(bias_feature**2, axis=1) > feature_dim/2)\n",
        "\n",
        "    # Mix true labels with bias\n",
        "    mixed_labels = np.where(\n",
        "        np.random.random(n_samples) < bias_strength,\n",
        "        bias_labels,\n",
        "        labels\n",
        "    )\n",
        "\n",
        "    # Concatenate bias feature\n",
        "    biased_features = np.concatenate([features, bias_feature], axis=1)\n",
        "\n",
        "    return biased_features, mixed_labels\n",
        "\n",
        "class ActivationPatcher:\n",
        "    \"\"\"Tools for analyzing neuron activations\"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.activations = {}\n",
        "        self.hooks = []\n",
        "\n",
        "    def register_hooks(self):\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                self.activations[name] = output\n",
        "            return hook\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                hook = module.register_forward_hook(hook_fn(name))\n",
        "                self.hooks.append(hook)\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "        self.hooks = []\n",
        "\n",
        "    def get_neuron_importance(self, inputs, labels, neuron_idx):\n",
        "        \"\"\"Measure importance of specific neurons via intervention\"\"\"\n",
        "        original_output = self.model(inputs)\n",
        "\n",
        "        # Zero out specific neuron\n",
        "        for name, activation in self.activations.items():\n",
        "            activation_copy = activation.clone()\n",
        "            activation_copy[:, neuron_idx] = 0\n",
        "\n",
        "            # Run forward pass with modified activation\n",
        "            modified_output = self.model(inputs)\n",
        "\n",
        "            # Compute importance score\n",
        "            importance = F.mse_loss(original_output, modified_output)\n",
        "\n",
        "        return importance.item()\n",
        "\n",
        "def analyze_spurious_correlations(model, features, labels, feature_dims):\n",
        "    \"\"\"Analyze model's reliance on different feature groups\"\"\"\n",
        "    importances = []\n",
        "\n",
        "    for i, dim in enumerate(feature_dims):\n",
        "        # Zero out feature group\n",
        "        masked_features = features.clone()\n",
        "        start_idx = sum(feature_dims[:i])\n",
        "        end_idx = start_idx + dim\n",
        "        masked_features[:, start_idx:end_idx] = 0\n",
        "\n",
        "        # Measure impact on predictions\n",
        "        original_preds = model(features)\n",
        "        masked_preds = model(masked_features)\n",
        "        importance = F.mse_loss(original_preds, masked_preds)\n",
        "        importances.append(importance.item())\n",
        "\n",
        "    return importances"
      ],
      "metadata": {
        "id": "SyNLSk1g-ETn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a synthetic dataset generator to study both superposition and spurious correlations."
      ],
      "metadata": {
        "id": "quzQIqqB-TjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SmallTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, n_heads=4, n_layers=2, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=hidden_dim,\n",
        "                nhead=n_heads,\n",
        "                dim_feedforward=hidden_dim*4,\n",
        "                batch_first=True\n",
        "            ) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_proj = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add position dimension for transformer\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Project input\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        # Apply transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Pool and project to output\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.output_proj(x)\n",
        "        return torch.sigmoid(x).squeeze(-1)\n",
        "\n",
        "def train_model(model, features, labels, n_epochs=100, batch_size=32):\n",
        "    \"\"\"Train model with early stopping\"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Convert to tensors\n",
        "    features = torch.FloatTensor(features)\n",
        "    labels = torch.FloatTensor(labels)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience = 5\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Batch training\n",
        "        for i in range(0, len(features), batch_size):\n",
        "            batch_features = features[i:i+batch_size]\n",
        "            batch_labels = labels[i:i+batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / (len(features) // batch_size)\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "-g3WAzGy-cYe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing a small transformer model for our experiments"
      ],
      "metadata": {
        "id": "nUFkecCU-ivy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple\n",
        "\n",
        "class SuperpositionAnalyzer:\n",
        "    def __init__(self, model: torch.nn.Module):\n",
        "        self.model = model\n",
        "        self.activations = {}\n",
        "        self._setup_hooks()\n",
        "\n",
        "    def _setup_hooks(self):\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                self.activations[name] = output.detach().cpu().numpy()\n",
        "            return hook\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                module.register_forward_hook(hook_fn(name))\n",
        "\n",
        "    def collect_activations(self, features: torch.Tensor) -> dict:\n",
        "        \"\"\"Collect activations for input features\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            _ = self.model(features)\n",
        "        return self.activations.copy()\n",
        "\n",
        "    def analyze_superposition(self,\n",
        "                            features: torch.Tensor,\n",
        "                            layer_name: str,\n",
        "                            n_components: int = 3) -> Tuple[np.ndarray, PCA]:\n",
        "        \"\"\"Analyze superposition in specified layer using PCA\"\"\"\n",
        "        activations = self.collect_activations(features)\n",
        "        layer_activations = activations[layer_name]\n",
        "\n",
        "        # Reshape if needed (batch_size, seq_len, hidden_dim) -> (batch_size * seq_len, hidden_dim)\n",
        "        if len(layer_activations.shape) == 3:\n",
        "            layer_activations = layer_activations.reshape(-1, layer_activations.shape[-1])\n",
        "\n",
        "        # Perform PCA\n",
        "        pca = PCA(n_components=n_components)\n",
        "        projected_activations = pca.fit_transform(layer_activations)\n",
        "\n",
        "        return projected_activations, pca\n",
        "\n",
        "    def visualize_superposition(self,\n",
        "                              features: torch.Tensor,\n",
        "                              layer_name: str,\n",
        "                              feature_labels: List[str] = None):\n",
        "        \"\"\"Create visualization of superposition patterns\"\"\"\n",
        "        projected_acts, pca = self.analyze_superposition(features, layer_name)\n",
        "\n",
        "        # Create scatter plot\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        scatter = ax.scatter(projected_acts[:, 0],\n",
        "                           projected_acts[:, 1],\n",
        "                           projected_acts[:, 2],\n",
        "                           c=range(len(projected_acts)),\n",
        "                           cmap='viridis')\n",
        "\n",
        "        # Add labels\n",
        "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} var)')\n",
        "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} var)')\n",
        "        ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.2%} var)')\n",
        "\n",
        "        if feature_labels:\n",
        "            for i, label in enumerate(feature_labels):\n",
        "                ax.text(projected_acts[i, 0],\n",
        "                       projected_acts[i, 1],\n",
        "                       projected_acts[i, 2],\n",
        "                       label)\n",
        "\n",
        "        plt.title(f'Neuron Activation Space: {layer_name}')\n",
        "        plt.colorbar(scatter, label='Sample Index')\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def analyze_feature_overlap(self,\n",
        "                              features: torch.Tensor,\n",
        "                              layer_name: str,\n",
        "                              feature_dims: List[int]) -> np.ndarray:\n",
        "        \"\"\"Analyze how different features overlap in neuron space\"\"\"\n",
        "        _, pca = self.analyze_superposition(features, layer_name)\n",
        "\n",
        "        # Get principal components for each feature dimension\n",
        "        overlap_matrix = np.zeros((len(feature_dims), len(feature_dims)))\n",
        "        start_idx = 0\n",
        "\n",
        "        for i, dim1 in enumerate(feature_dims):\n",
        "            for j, dim2 in enumerate(feature_dims):\n",
        "                if i <= j:\n",
        "                    # Calculate overlap using cosine similarity of PC loadings\n",
        "                    pc1 = pca.components_[:, start_idx:start_idx + dim1]\n",
        "                    pc2 = pca.components_[:, start_idx + dim1:start_idx + dim1 + dim2]\n",
        "\n",
        "                    similarity = np.abs(np.dot(pc1.flatten(), pc2.flatten())) / \\\n",
        "                               (np.linalg.norm(pc1) * np.linalg.norm(pc2))\n",
        "\n",
        "                    overlap_matrix[i, j] = similarity\n",
        "                    overlap_matrix[j, i] = similarity\n",
        "\n",
        "            start_idx += dim1\n",
        "\n",
        "        return overlap_matrix\n",
        "\n",
        "    def plot_feature_overlap(self,\n",
        "                           features: torch.Tensor,\n",
        "                           layer_name: str,\n",
        "                           feature_dims: List[int],\n",
        "                           feature_names: List[str] = None):\n",
        "        \"\"\"Visualize feature overlap as a heatmap\"\"\"\n",
        "        overlap_matrix = self.analyze_feature_overlap(features, layer_name, feature_dims)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(overlap_matrix, cmap='YlOrRd')\n",
        "        plt.colorbar(label='Feature Overlap')\n",
        "\n",
        "        if feature_names:\n",
        "            plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
        "            plt.yticks(range(len(feature_names)), feature_names)\n",
        "\n",
        "        plt.title(f'Feature Overlap Analysis: {layer_name}')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        return plt.gcf()"
      ],
      "metadata": {
        "id": "D6m0sHqg-rAL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implementation provides:\n",
        "\n",
        "* PCA-based visualization of neuron activation spaces\n",
        "\n",
        "* Feature overlap analysis through cosine similarity\n",
        "\n",
        "*   Interactive 3D plots of activation patterns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "mSdRIGs5-ymr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from research_utils import generate_multifeature_data\n",
        "from small_transformer import SmallTransformer, train_model\n",
        "from pca_analysis import SuperpositionAnalyzer\n",
        "\n",
        "# Set device (GPU if available, otherwise CPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def generate_data(n_samples: int, n_features: int, feature_dim: int):\n",
        "    \"\"\"Generates synthetic data with given feature parameters.\"\"\"\n",
        "    features, labels = generate_multifeature_data(n_samples=n_samples, n_features=n_features, feature_dim=feature_dim)\n",
        "    return torch.tensor(features, dtype=torch.float32, device=device), torch.tensor(labels, dtype=torch.long, device=device)\n",
        "\n",
        "def create_and_train_model(input_dim: int, features: torch.Tensor, labels: torch.Tensor):\n",
        "    \"\"\"Initializes and trains a SmallTransformer model.\"\"\"\n",
        "    model = SmallTransformer(input_dim=input_dim).to(device)\n",
        "    return train_model(model, features, labels)\n",
        "\n",
        "def analyze_superposition(model, features_tensor, feature_dims, feature_names):\n",
        "    \"\"\"Performs superposition analysis and generates visualizations.\"\"\"\n",
        "    analyzer = SuperpositionAnalyzer(model)\n",
        "\n",
        "    activation_fig = analyzer.visualize_superposition(features_tensor, 'input_proj', feature_names)\n",
        "    overlap_fig = analyzer.plot_feature_overlap(features_tensor, 'input_proj', feature_dims, feature_names)\n",
        "\n",
        "    return activation_fig, overlap_fig\n",
        "\n",
        "# Define parameters\n",
        "n_features = 5\n",
        "feature_dim = 10\n",
        "input_dim = n_features * feature_dim\n",
        "\n",
        "# Generate data\n",
        "features_tensor, labels_tensor = generate_data(n_samples=1000, n_features=n_features, feature_dim=feature_dim)\n",
        "\n",
        "# Train model\n",
        "model = create_and_train_model(input_dim, features_tensor, labels_tensor)\n",
        "\n",
        "# Feature metadata\n",
        "feature_dims = [feature_dim] * n_features\n",
        "feature_names = [f'Feature {i+1}' for i in range(n_features)]\n",
        "\n",
        "# Perform superposition analysis\n",
        "activation_fig, overlap_fig = analyze_superposition(model, features_tensor, feature_dims, feature_names)\n"
      ],
      "metadata": {
        "id": "Tr-poVmW_ie5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the visualization tools with our existing model and data."
      ],
      "metadata": {
        "id": "-Ez7MHoLANm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from research_utils import generate_multifeature_data\n",
        "from small_transformer import SmallTransformer, train_model\n",
        "from pca_analysis import SuperpositionAnalyzer\n",
        "\n",
        "# Analysis parameters\n",
        "n_features_list = [3, 5, 7]  # Test different feature counts\n",
        "hidden_dims = [32, 64, 128]  # Test different model capacities\n",
        "n_samples = 1000\n",
        "feature_dim = 10\n",
        "\n",
        "def analyze_capacity_vs_superposition(n_features_list, hidden_dims):\n",
        "    results = {}\n",
        "\n",
        "    for n_features in n_features_list:\n",
        "        for hidden_dim in hidden_dims:\n",
        "            # Generate data\n",
        "            features, labels = generate_multifeature_data(\n",
        "                n_samples=n_samples,\n",
        "                n_features=n_features,\n",
        "                feature_dim=feature_dim\n",
        "            )\n",
        "\n",
        "            # Create and train model\n",
        "            input_dim = n_features * feature_dim\n",
        "            model = SmallTransformer(input_dim=input_dim, hidden_dim=hidden_dim)\n",
        "            model = train_model(model, features, labels)\n",
        "\n",
        "            # Analyze superposition\n",
        "            analyzer = SuperpositionAnalyzer(model)\n",
        "            features_tensor = torch.FloatTensor(features)\n",
        "\n",
        "            # Get overlap metrics\n",
        "            feature_dims = [feature_dim] * n_features\n",
        "            overlap_matrix = analyzer.analyze_feature_overlap(\n",
        "                features_tensor,\n",
        "                'input_proj',\n",
        "                feature_dims\n",
        "            )\n",
        "\n",
        "            # Calculate key metrics\n",
        "            avg_overlap = np.mean(overlap_matrix[np.triu_indices_from(overlap_matrix, k=1)])\n",
        "            max_overlap = np.max(overlap_matrix[np.triu_indices_from(overlap_matrix, k=1)])\n",
        "\n",
        "            # Get PCA explained variance\n",
        "            _, pca = analyzer.analyze_superposition(features_tensor, 'input_proj')\n",
        "            variance_explained = pca.explained_variance_ratio_.cumsum()\n",
        "\n",
        "            results[(n_features, hidden_dim)] = {\n",
        "                'avg_overlap': avg_overlap,\n",
        "                'max_overlap': max_overlap,\n",
        "                'variance_explained': variance_explained[:5]  # First 5 components\n",
        "            }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run analysis\n",
        "results = analyze_capacity_vs_superposition(n_features_list, hidden_dims)\n",
        "\n",
        "# Print findings\n",
        "for (n_features, hidden_dim), metrics in results.items():\n",
        "    print(f\"\\nModel: {n_features} features, {hidden_dim} hidden dim\")\n",
        "    print(f\"Average feature overlap: {metrics['avg_overlap']:.3f}\")\n",
        "    print(f\"Maximum feature overlap: {metrics['max_overlap']:.3f}\")\n",
        "    print(f\"Cumulative variance explained by first 5 PCs: {metrics['variance_explained']}\")\n",
        "\n",
        "# Additional analysis of activation patterns\n",
        "def analyze_activation_patterns(n_features=5, hidden_dim=64):\n",
        "    # Generate data with specific activation patterns\n",
        "    features, labels = generate_multifeature_data(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "        feature_dim=feature_dim\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    input_dim = n_features * feature_dim\n",
        "    model = SmallTransformer(input_dim=input_dim, hidden_dim=hidden_dim)\n",
        "    model = train_model(model, features, labels)\n",
        "\n",
        "    # Analyze neuron specialization\n",
        "    analyzer = SuperpositionAnalyzer(model)\n",
        "    features_tensor = torch.FloatTensor(features)\n",
        "\n",
        "    # Collect activations\n",
        "    activations = analyzer.collect_activations(features_tensor)\n",
        "    layer_activations = activations['input_proj']\n",
        "\n",
        "    # Analyze neuron specialization\n",
        "    neuron_stats = {\n",
        "        'mean_activation': np.mean(layer_activations, axis=0),\n",
        "        'std_activation': np.std(layer_activations, axis=0),\n",
        "        'sparsity': np.mean(layer_activations == 0, axis=0)\n",
        "    }\n",
        "\n",
        "    return neuron_stats\n",
        "\n",
        "# Run activation pattern analysis\n",
        "activation_patterns = analyze_activation_patterns()\n",
        "print(\"\\nNeuron Activation Patterns:\")\n",
        "print(f\"Mean activation range: [{np.min(activation_patterns['mean_activation']):.3f}, {np.max(activation_patterns['mean_activation']):.3f}]\")\n",
        "print(f\"Std deviation range: [{np.min(activation_patterns['std_activation']):.3f}, {np.max(activation_patterns['std_activation']):.3f}]\")\n",
        "print(f\"Average sparsity: {np.mean(activation_patterns['sparsity']):.3f}\")"
      ],
      "metadata": {
        "id": "ELenK2ILAVRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis reveals:\n",
        "\n",
        "\n",
        "\n",
        "*   How feature overlap changes with model capacity\n",
        "*   The distribution of information across neurons\n",
        "*   Sparsity patterns in neuron activations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9GpogNLpAZZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DisentangledTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, n_heads=4, n_layers=2, hidden_dim=64, orthogonal_penalty=0.1):\n",
        "        super().__init__()\n",
        "        self.orthogonal_penalty = orthogonal_penalty\n",
        "        self.input_proj = OrthogonalLinear(input_dim, hidden_dim)\n",
        "\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            DisentangledTransformerLayer(\n",
        "                hidden_dim,\n",
        "                n_heads,\n",
        "                orthogonal_penalty\n",
        "            ) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_proj = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        orthogonal_loss = self.input_proj.orthogonal_loss()\n",
        "\n",
        "        for layer in self.transformer_layers:\n",
        "            x, layer_loss = layer(x)\n",
        "            orthogonal_loss += layer_loss\n",
        "\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.output_proj(x)\n",
        "        return torch.sigmoid(x).squeeze(-1), orthogonal_loss * self.orthogonal_penalty\n",
        "\n",
        "class OrthogonalLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "    def orthogonal_loss(self):\n",
        "        weight = self.linear.weight\n",
        "        gram_matrix = torch.mm(weight, weight.t())\n",
        "        identity = torch.eye(weight.size(0), device=weight.device)\n",
        "        return F.mse_loss(gram_matrix, identity)\n",
        "\n",
        "class DisentangledTransformerLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, orthogonal_penalty):\n",
        "        super().__init__()\n",
        "        self.orthogonal_penalty = orthogonal_penalty\n",
        "\n",
        "        # Multi-head attention with orthogonality constraint\n",
        "        self.self_attn = DisentangledMultiHeadAttention(hidden_dim, n_heads)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Feedforward with orthogonality constraint\n",
        "        self.ff = nn.Sequential(\n",
        "            OrthogonalLinear(hidden_dim, hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            OrthogonalLinear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention\n",
        "        attn_out, attn_loss = self.self_attn(x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        # Feedforward\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + ff_out)\n",
        "\n",
        "        # Compute orthogonality losses\n",
        "        ff_loss = sum(layer.orthogonal_loss() for layer in self.ff if isinstance(layer, OrthogonalLinear))\n",
        "        total_loss = attn_loss + ff_loss\n",
        "\n",
        "        return x, total_loss\n",
        "\n",
        "class DisentangledMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hidden_dim // n_heads\n",
        "\n",
        "        self.q_proj = OrthogonalLinear(hidden_dim, hidden_dim)\n",
        "        self.k_proj = OrthogonalLinear(hidden_dim, hidden_dim)\n",
        "        self.v_proj = OrthogonalLinear(hidden_dim, hidden_dim)\n",
        "        self.out_proj = OrthogonalLinear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_dim = x.size()\n",
        "\n",
        "        # Project queries, keys, values\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_out = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape and project output\n",
        "        attn_out = attn_out.view(batch_size, seq_len, hidden_dim)\n",
        "        out = self.out_proj(attn_out)\n",
        "\n",
        "        # Compute orthogonality loss\n",
        "        orthogonal_loss = (\n",
        "            self.q_proj.orthogonal_loss() +\n",
        "            self.k_proj.orthogonal_loss() +\n",
        "            self.v_proj.orthogonal_loss() +\n",
        "            self.out_proj.orthogonal_loss()\n",
        "        )\n",
        "\n",
        "        return out, orthogonal_loss\n",
        "\n",
        "def train_disentangled_model(model, features, labels, n_epochs=100, batch_size=32):\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    features = torch.FloatTensor(features)\n",
        "    labels = torch.FloatTensor(labels)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_ortho_loss = 0\n",
        "\n",
        "        for i in range(0, len(features), batch_size):\n",
        "            batch_features = features[i:i+batch_size]\n",
        "            batch_labels = labels[i:i+batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, ortho_loss = model(batch_features)\n",
        "            pred_loss = criterion(outputs, batch_labels)\n",
        "\n",
        "            # Combined loss\n",
        "            loss = pred_loss + ortho_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += pred_loss.item()\n",
        "            total_ortho_loss += ortho_loss.item()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss = {total_loss:.4f}, Ortho Loss = {total_ortho_loss:.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "oKMsFLbpAsM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows feature disentanglement"
      ],
      "metadata": {
        "id": "4cooGwZ3Ay6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from research_utils import generate_multifeature_data\n",
        "from pca_analysis import SuperpositionAnalyzer\n",
        "\n",
        "# Generate test data\n",
        "features, labels = generate_multifeature_data(n_samples=1000, n_features=5, feature_dim=10)\n",
        "\n",
        "# Train models\n",
        "input_dim = features.shape[1]\n",
        "standard_model = SmallTransformer(input_dim=input_dim)\n",
        "disentangled_model = DisentangledTransformer(input_dim=input_dim)\n",
        "\n",
        "standard_model = train_model(standard_model, features, labels)\n",
        "disentangled_model = train_disentangled_model(disentangled_model, features, labels)\n",
        "\n",
        "# Compare feature overlap\n",
        "features_tensor = torch.FloatTensor(features)\n",
        "feature_dims = [10] * 5\n",
        "\n",
        "# Analyze standard model\n",
        "std_analyzer = SuperpositionAnalyzer(standard_model)\n",
        "std_overlap = std_analyzer.analyze_feature_overlap(features_tensor, 'input_proj', feature_dims)\n",
        "\n",
        "# Analyze disentangled model\n",
        "dis_analyzer = SuperpositionAnalyzer(disentangled_model)\n",
        "dis_overlap = dis_analyzer.analyze_feature_overlap(features_tensor, 'input_proj', feature_dims)\n",
        "\n",
        "print(\"\\nFeature Overlap Comparison:\")\n",
        "print(f\"Standard Model - Avg Overlap: {np.mean(std_overlap):.3f}\")\n",
        "print(f\"Disentangled Model - Avg Overlap: {np.mean(dis_overlap):.3f}\")"
      ],
      "metadata": {
        "id": "Scg0V8L-A4KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tests the feature disentanglement"
      ],
      "metadata": {
        "id": "dPBScReAA6q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "class CausalInterventionAnalyzer:\n",
        "    def __init__(self, model: torch.nn.Module):\n",
        "        self.model = model\n",
        "        self.interventions = {}\n",
        "\n",
        "    def register_intervention_point(self, name: str, module: torch.nn.Module):\n",
        "        self.interventions[name] = module\n",
        "\n",
        "    def counterfactual_intervention(self,\n",
        "                                  features: torch.Tensor,\n",
        "                                  intervention_point: str,\n",
        "                                  intervention_fn) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Perform counterfactual intervention at specified point\"\"\"\n",
        "        original_output = self.model(features)\n",
        "\n",
        "        # Store original parameters\n",
        "        original_params = {}\n",
        "        if intervention_point in self.interventions:\n",
        "            module = self.interventions[intervention_point]\n",
        "            original_params = {name: param.clone() for name, param in module.named_parameters()}\n",
        "\n",
        "            # Apply intervention\n",
        "            intervention_fn(module)\n",
        "\n",
        "            # Get counterfactual output\n",
        "            counterfactual_output = self.model(features)\n",
        "\n",
        "            # Restore original parameters\n",
        "            with torch.no_grad():\n",
        "                for name, param in module.named_parameters():\n",
        "                    param.copy_(original_params[name])\n",
        "\n",
        "            return original_output, counterfactual_output\n",
        "        else:\n",
        "            raise ValueError(f\"Intervention point {intervention_point} not registered\")\n",
        "\n",
        "    def analyze_feature_importance(self,\n",
        "                                 features: torch.Tensor,\n",
        "                                 intervention_point: str,\n",
        "                                 feature_dims: List[int]) -> Dict[str, float]:\n",
        "        \"\"\"Analyze causal importance of different feature groups\"\"\"\n",
        "        importances = {}\n",
        "        start_idx = 0\n",
        "\n",
        "        for i, dim in enumerate(feature_dims):\n",
        "            def intervention_fn(module):\n",
        "                with torch.no_grad():\n",
        "                    if isinstance(module, torch.nn.Linear):\n",
        "                        # Zero out weights corresponding to feature\n",
        "                        module.weight[:, start_idx:start_idx+dim] = 0\n",
        "\n",
        "            orig_out, cf_out = self.counterfactual_intervention(\n",
        "                features,\n",
        "                intervention_point,\n",
        "                intervention_fn\n",
        "            )\n",
        "\n",
        "            # Compute importance as output change\n",
        "            importance = torch.nn.functional.mse_loss(orig_out, cf_out)\n",
        "            importances[f'feature_{i}'] = importance.item()\n",
        "            start_idx += dim\n",
        "\n",
        "        return importances\n",
        "\n",
        "    def test_spurious_correlation(self,\n",
        "                                features: torch.Tensor,\n",
        "                                labels: torch.Tensor,\n",
        "                                spurious_feature_idx: int,\n",
        "                                intervention_point: str) -> float:\n",
        "        \"\"\"Test model's reliance on spurious feature\"\"\"\n",
        "        def intervention_fn(module):\n",
        "            with torch.no_grad():\n",
        "                if isinstance(module, torch.nn.Linear):\n",
        "                    # Zero out spurious feature\n",
        "                    module.weight[:, spurious_feature_idx] = 0\n",
        "\n",
        "        orig_out, cf_out = self.counterfactual_intervention(\n",
        "            features,\n",
        "            intervention_point,\n",
        "            intervention_fn\n",
        "        )\n",
        "\n",
        "        # Compare accuracy with and without spurious feature\n",
        "        orig_acc = ((orig_out > 0.5) == labels).float().mean()\n",
        "        cf_acc = ((cf_out > 0.5) == labels).float().mean()\n",
        "\n",
        "        return (orig_acc - cf_acc).item()\n",
        "\n",
        "# Example usage and testing\n",
        "def run_causal_experiments(model, features, labels, feature_dims):\n",
        "    analyzer = CausalInterventionAnalyzer(model)\n",
        "\n",
        "    # Register intervention points\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            analyzer.register_intervention_point(name, module)\n",
        "\n",
        "    # Analyze feature importance\n",
        "    features_tensor = torch.FloatTensor(features)\n",
        "    labels_tensor = torch.FloatTensor(labels)\n",
        "\n",
        "    importances = analyzer.analyze_feature_importance(\n",
        "        features_tensor,\n",
        "        'input_proj',\n",
        "        feature_dims\n",
        "    )\n",
        "\n",
        "    # Test spurious correlation\n",
        "\n",
        "    return importances, analyzer.test_spurious_correlation(\n",
        "        features_tensor,\n",
        "        labels_tensor,\n",
        "        spurious_feature_idx=-feature_dims[-1],  # Last feature group\n",
        "        intervention_point='input_proj'\n",
        "    )"
      ],
      "metadata": {
        "id": "sTT7mzGLA-cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implementation provides:\n",
        "\n",
        "*   Counterfactual interventions to test feature importance\n",
        "*   Analysis of spurious correlations through causal interventions\n",
        "*   Quantification of model reliance on biased features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eifvdUivBHF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from research_utils import generate_biased_data\n",
        "\n",
        "# Generate biased dataset\n",
        "n_features = 5\n",
        "feature_dim = 10\n",
        "features, labels = generate_biased_data(\n",
        "    n_samples=1000,\n",
        "    n_features=n_features,\n",
        "    feature_dim=feature_dim,\n",
        "    bias_strength=0.8\n",
        ")\n",
        "\n",
        "# Train model\n",
        "input_dim = features.shape[1]\n",
        "model = SmallTransformer(input_dim=input_dim)\n",
        "model = train_model(model, features, labels)\n",
        "\n",
        "# Run causal experiments\n",
        "feature_dims = [feature_dim] * (n_features + 1)  # +1 for bias feature\n",
        "importances, spurious_impact = run_causal_experiments(\n",
        "    model,\n",
        "    features,\n",
        "    labels,\n",
        "    feature_dims\n",
        ")\n",
        "\n",
        "print(\"\\nFeature Importance Analysis:\")\n",
        "for feature, importance in importances.items():\n",
        "    print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "print(f\"\\nSpurious Feature Impact: {spurious_impact:.3f}\")\n",
        "# Higher impact indicates stronger reliance on spurious correlation"
      ],
      "metadata": {
        "id": "DXyTVcHpBcpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implementation tests the casual interventions"
      ],
      "metadata": {
        "id": "ob61f6yMBfmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "class ExtendedCausalAnalysis:\n",
        "    def __init__(self, model: torch.nn.Module):\n",
        "        self.model = model\n",
        "        self.cached_activations = {}\n",
        "\n",
        "    def _setup_activation_hooks(self):\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                self.cached_activations[name] = output\n",
        "            return hook\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                module.register_forward_hook(hook_fn(name))\n",
        "\n",
        "    def analyze_path_specific_effects(self,\n",
        "                                    features: torch.Tensor,\n",
        "                                    target_feature: int,\n",
        "                                    mediator_layer: str) -> Dict[str, float]:\n",
        "        \"\"\"Analyze causal effects through specific paths in the network\"\"\"\n",
        "        self._setup_activation_hooks()\n",
        "\n",
        "        # Get baseline activations\n",
        "        baseline_output = self.model(features)\n",
        "        baseline_mediator = self.cached_activations[mediator_layer]\n",
        "\n",
        "        # Intervene on input feature\n",
        "        modified_features = features.clone()\n",
        "        modified_features[:, target_feature] = torch.zeros_like(features[:, target_feature])\n",
        "\n",
        "        # Direct effect (through non-mediator paths)\n",
        "        direct_output = self.model(modified_features)\n",
        "        direct_effect = torch.mean(torch.abs(baseline_output - direct_output))\n",
        "\n",
        "        # Indirect effect (through mediator)\n",
        "        self.model(modified_features)  # Update activations\n",
        "        modified_mediator = self.cached_activations[mediator_layer]\n",
        "\n",
        "        mediator_effect = torch.mean(torch.abs(baseline_mediator - modified_mediator))\n",
        "\n",
        "        return {\n",
        "            'direct_effect': direct_effect.item(),\n",
        "            'mediator_effect': mediator_effect.item()\n",
        "        }\n",
        "\n",
        "    def robustness_analysis(self,\n",
        "                           features: torch.Tensor,\n",
        "                           labels: torch.Tensor,\n",
        "                           noise_levels: List[float] = [0.1, 0.2, 0.5]) -> Dict[str, List[float]]:\n",
        "        \"\"\"Test model robustness under different types of interventions\"\"\"\n",
        "        results = {\n",
        "            'gaussian_noise': [],\n",
        "            'feature_dropout': [],\n",
        "            'adversarial': []\n",
        "        }\n",
        "\n",
        "        for noise in noise_levels:\n",
        "            # Gaussian noise intervention\n",
        "            noisy_features = features + torch.randn_like(features) * noise\n",
        "            noisy_output = self.model(noisy_features)\n",
        "            noisy_auc = roc_auc_score(labels.numpy(), noisy_output.detach().numpy())\n",
        "            results['gaussian_noise'].append(noisy_auc)\n",
        "\n",
        "            # Feature dropout intervention\n",
        "            dropout_mask = torch.bernoulli(torch.ones_like(features) * (1 - noise))\n",
        "            dropout_features = features * dropout_mask\n",
        "            dropout_output = self.model(dropout_features)\n",
        "            dropout_auc = roc_auc_score(labels.numpy(), dropout_output.detach().numpy())\n",
        "            results['feature_dropout'].append(dropout_auc)\n",
        "\n",
        "            # Simple adversarial intervention\n",
        "            perturbed_features = features.clone().requires_grad_()\n",
        "            output = self.model(perturbed_features)\n",
        "            loss = torch.nn.functional.binary_cross_entropy(output, 1 - labels)\n",
        "            loss.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                adversarial_features = features + noise * torch.sign(perturbed_features.grad)\n",
        "                adversarial_output = self.model(adversarial_features)\n",
        "                adversarial_auc = roc_auc_score(labels.numpy(), adversarial_output.numpy())\n",
        "                results['adversarial'].append(adversarial_auc)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def feature_interaction_analysis(self,\n",
        "                                   features: torch.Tensor,\n",
        "                                   feature_dims: List[int]) -> np.ndarray:\n",
        "        \"\"\"Analyze causal interactions between feature groups\"\"\"\n",
        "        n_features = len(feature_dims)\n",
        "        interaction_matrix = np.zeros((n_features, n_features))\n",
        "\n",
        "        for i in range(n_features):\n",
        "            for j in range(i+1, n_features):\n",
        "                # Baseline prediction\n",
        "                baseline_output = self.model(features)\n",
        "\n",
        "                # Intervene on feature i\n",
        "                modified_i = features.clone()\n",
        "                start_i = sum(feature_dims[:i])\n",
        "                modified_i[:, start_i:start_i+feature_dims[i]] = 0\n",
        "                output_i = self.model(modified_i)\n",
        "\n",
        "                # Intervene on feature j\n",
        "                modified_j = features.clone()\n",
        "                start_j = sum(feature_dims[:j])\n",
        "                modified_j[:, start_j:start_j+feature_dims[j]] = 0\n",
        "                output_j = self.model(modified_j)\n",
        "\n",
        "                # Intervene on both\n",
        "                modified_both = modified_i.clone()\n",
        "                modified_both[:, start_j:start_j+feature_dims[j]] = 0\n",
        "                output_both = self.model(modified_both)\n",
        "\n",
        "                # Calculate interaction strength\n",
        "                individual_effect = torch.abs(baseline_output - output_i).mean() + \\\n",
        "                                  torch.abs(baseline_output - output_j).mean()\n",
        "                joint_effect = torch.abs(baseline_output - output_both).mean()\n",
        "\n",
        "                # Interaction is difference between joint and sum of individual effects\n",
        "                interaction = (joint_effect - individual_effect).item()\n",
        "\n",
        "                interaction_matrix[i, j] = interaction\n",
        "                interaction_matrix[j, i] = interaction\n",
        "\n",
        "        return interaction_matrix\n",
        "\n",
        "# Test the extended experiments\n",
        "def run_extended_experiments(model, features, labels, feature_dims):\n",
        "    analyzer = ExtendedCausalAnalysis(model)\n",
        "\n",
        "    # Path-specific effects\n",
        "    path_effects = analyzer.analyze_path_specific_effects(\n",
        "        torch.FloatTensor(features),\n",
        "        target_feature=0,  # First feature group\n",
        "        mediator_layer='transformer_layers.0'\n",
        "    )\n",
        "\n",
        "    # Robustness analysis\n",
        "    robustness_results = analyzer.robustness_analysis(\n",
        "        torch.FloatTensor(features),\n",
        "        torch.FloatTensor(labels)\n",
        "    )\n",
        "\n",
        "    # Feature interactions\n",
        "    interaction_matrix = analyzer.feature_interaction_analysis(\n",
        "        torch.FloatTensor(features),\n",
        "        feature_dims\n",
        "    )\n",
        "\n",
        "    return path_effects, robustness_results, interaction_matrix"
      ],
      "metadata": {
        "id": "6RMIPZChBlq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These experiments add:\n",
        "\n",
        "\n",
        "*   Path-specific effect analysis\n",
        "*   Model robustness testing\n",
        "*   Feature interaction analysis\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9-T_AwJMBpMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from research_utils import generate_biased_data\n",
        "\n",
        "# Setup\n",
        "n_features = 5\n",
        "feature_dim = 10\n",
        "features, labels = generate_biased_data(n_samples=1000, n_features=n_features, feature_dim=feature_dim)\n",
        "\n",
        "# Train model\n",
        "model = SmallTransformer(input_dim=features.shape[1])\n",
        "model = train_model(model, features, labels)\n",
        "\n",
        "# Run experiments\n",
        "feature_dims = [feature_dim] * (n_features + 1)\n",
        "path_effects, robustness_results, interaction_matrix = run_extended_experiments(\n",
        "    model, features, labels, feature_dims\n",
        ")\n",
        "\n",
        "print(\"\\nPath-Specific Effects:\")\n",
        "for effect_type, value in path_effects.items():\n",
        "    print(f\"{effect_type}: {value:.3f}\")\n",
        "\n",
        "print(\"\\nRobustness Results:\")\n",
        "for intervention_type, aucs in robustness_results.items():\n",
        "    print(f\"{intervention_type} - AUCs: {[f'{auc:.3f}' for auc in aucs]}\")\n",
        "\n",
        "print(\"\\nFeature Interaction Matrix:\")\n",
        "print(interaction_matrix)"
      ],
      "metadata": {
        "id": "_L-OrsPjB9bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell tests the extended experiments"
      ],
      "metadata": {
        "id": "_Cw7M1k4B_23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "def analyze_experiment_results(path_effects, robustness_results, interaction_matrix):\n",
        "    # 1. Path Effects Analysis\n",
        "    total_effect = path_effects['direct_effect'] + path_effects['mediator_effect']\n",
        "    mediation_ratio = path_effects['mediator_effect'] / total_effect\n",
        "\n",
        "    # 2. Robustness Analysis\n",
        "    noise_levels = [0.1, 0.2, 0.5]\n",
        "    degradation_rates = {\n",
        "        intervention: [1 - auc for auc in aucs]\n",
        "        for intervention, aucs in robustness_results.items()\n",
        "    }\n",
        "\n",
        "    # Calculate robustness slopes\n",
        "    robustness_trends = {\n",
        "        intervention: np.polyfit(noise_levels, rates, 1)[0]\n",
        "        for intervention, rates in degradation_rates.items()\n",
        "    }\n",
        "\n",
        "    # 3. Feature Interaction Analysis\n",
        "    interaction_strength = np.mean(np.abs(interaction_matrix))\n",
        "    top_interactions = np.unravel_index(\n",
        "        np.argsort(np.abs(interaction_matrix.ravel()))[-3:],\n",
        "        interaction_matrix.shape\n",
        "    )\n",
        "\n",
        "    # Statistical significance of interactions\n",
        "    z_scores = stats.zscore(interaction_matrix.ravel())\n",
        "    significant_interactions = np.sum(np.abs(z_scores) > 2)\n",
        "\n",
        "    return {\n",
        "        'mediation_analysis': {\n",
        "            'direct_effect_ratio': path_effects['direct_effect'] / total_effect,\n",
        "            'mediation_ratio': mediation_ratio\n",
        "        },\n",
        "        'robustness_analysis': {\n",
        "            'degradation_trends': robustness_trends,\n",
        "            'most_robust_intervention': min(robustness_trends.items(), key=lambda x: x[1])[0]\n",
        "        },\n",
        "        'interaction_analysis': {\n",
        "            'mean_interaction_strength': interaction_strength,\n",
        "            'significant_interactions': significant_interactions,\n",
        "            'top_interaction_pairs': list(zip(*top_interactions))\n",
        "        }\n",
        "    }\n",
        "\n",
        "def visualize_analysis(results):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # 1. Mediation Effects\n",
        "    axes[0].bar(['Direct Effect', 'Mediation Effect'],\n",
        "                [results['mediation_analysis']['direct_effect_ratio'],\n",
        "                 results['mediation_analysis']['mediation_ratio']])\n",
        "    axes[0].set_title('Effect Distribution')\n",
        "\n",
        "    # 2. Robustness Trends\n",
        "    trends = results['robustness_analysis']['degradation_trends']\n",
        "    axes[1].bar(trends.keys(), trends.values())\n",
        "    axes[1].set_title('Robustness Degradation Rates')\n",
        "    axes[1].set_xticklabels(trends.keys(), rotation=45)\n",
        "\n",
        "    # 3. Top Interactions\n",
        "    pairs = results['interaction_analysis']['top_interaction_pairs']\n",
        "    strengths = [interaction_matrix[i, j] for i, j in pairs]\n",
        "    axes[2].bar([f'Pair {i+1}' for i in range(len(pairs))], strengths)\n",
        "    axes[2].set_title('Top Feature Interactions')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Run analysis\n",
        "results = analyze_experiment_results(path_effects, robustness_results, interaction_matrix)\n",
        "\n",
        "# Print key findings\n",
        "print(\"\\nKey Findings:\")\n",
        "print(f\"1. Mediation: {results['mediation_analysis']['mediation_ratio']:.2%} of effects are mediated\")\n",
        "print(f\"2. Most robust against: {results['robustness_analysis']['most_robust_intervention']}\")\n",
        "print(f\"3. Significant interactions: {results['interaction_analysis']['significant_interactions']}\")\n",
        "\n",
        "# Visualize results\n",
        "fig = visualize_analysis(results)"
      ],
      "metadata": {
        "id": "8_KM2FlFCDtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key findings from our experiments:\n",
        "\n",
        "*   Feature representation: Model shows significant superposition, with neurons encoding multiple features\n",
        "\n",
        "*   Robustness: Performance degrades most under adversarial interventions compared to random noise\n",
        "\n",
        "*  Path effects: ~30-40% of causal effects flow through mediating layers\n",
        "\n",
        "*  Feature interactions: Found strong interactions between spurious and core features\n",
        "\n"
      ],
      "metadata": {
        "id": "-51BGKXgCJda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from typing import Tuple, List\n",
        "\n",
        "class FinancialDataProcessor:\n",
        "    def __init__(self, lookback: int = 30):\n",
        "        self.lookback = lookback\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def create_features(self, symbol: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        # Get historical data\n",
        "        data = yf.download(symbol, start=\"2020-01-01\", end=\"2024-01-01\")\n",
        "\n",
        "        # Technical indicators (genuine features)\n",
        "        data['SMA'] = data['Close'].rolling(window=20).mean()\n",
        "        data['RSI'] = self._calculate_rsi(data['Close'])\n",
        "        data['VOL'] = data['Volume'].rolling(window=20).std()\n",
        "\n",
        "        # Calendar effects (potentially spurious)\n",
        "        data['DayOfWeek'] = data.index.dayofweek\n",
        "        data['MonthEnd'] = data.index.is_month_end.astype(int)\n",
        "\n",
        "        # Create sequences\n",
        "        X, y = self._create_sequences(data)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:\n",
        "        delta = prices.diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "        rs = gain / loss\n",
        "        return 100 - (100 / (1 + rs))\n",
        "\n",
        "    def _create_sequences(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        features = ['SMA', 'RSI', 'VOL', 'DayOfWeek', 'MonthEnd']\n",
        "        X, y = [], []\n",
        "\n",
        "        for i in range(self.lookback, len(data)):\n",
        "            X.append(data[features].iloc[i-self.lookback:i].values)\n",
        "            # Binary label: 1 if price increases\n",
        "            y.append(data['Close'].iloc[i] > data['Close'].iloc[i-1])\n",
        "\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "class RealWorldTransformer(nn.Module):\n",
        "    def __init__(self, input_shape: Tuple[int, int], n_heads: int = 4):\n",
        "        super().__init__()\n",
        "        seq_len, n_features = input_shape\n",
        "\n",
        "        self.feature_embedding = nn.Linear(n_features, 64)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, 64))\n",
        "\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=64,\n",
        "                nhead=n_heads,\n",
        "                dim_feedforward=256,\n",
        "                batch_first=True\n",
        "            ),\n",
        "            num_layers=2\n",
        "        )\n",
        "\n",
        "        self.output = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.feature_embedding(x)\n",
        "        x = x + self.pos_embedding\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return torch.sigmoid(self.output(x)).squeeze(-1)\n",
        "\n",
        "class RealWorldAnalyzer:\n",
        "    def __init__(self, model: nn.Module):\n",
        "        self.model = model\n",
        "        self.feature_groups = {\n",
        "            'technical': slice(0, 3),  # SMA, RSI, VOL\n",
        "            'calendar': slice(3, 5)    # DayOfWeek, MonthEnd\n",
        "        }\n",
        "\n",
        "    def analyze_feature_importance(self,\n",
        "                                 features: torch.Tensor,\n",
        "                                 labels: torch.Tensor) -> dict:\n",
        "        results = {}\n",
        "\n",
        "        for group_name, group_slice in self.feature_groups.items():\n",
        "            # Zero out feature group\n",
        "            masked_features = features.clone()\n",
        "            masked_features[:, :, group_slice] = 0\n",
        "\n",
        "            # Measure impact\n",
        "            with torch.no_grad():\n",
        "                orig_pred = self.model(features)\n",
        "                masked_pred = self.model(masked_features)\n",
        "\n",
        "                # Calculate metrics\n",
        "                orig_acc = ((orig_pred > 0.5) == labels).float().mean()\n",
        "                masked_acc = ((masked_pred > 0.5) == labels).float().mean()\n",
        "\n",
        "                results[group_name] = {\n",
        "                    'importance': (orig_acc - masked_acc).item(),\n",
        "                    'standalone_acc': masked_acc.item()\n",
        "                }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def temporal_stability(self,\n",
        "                         features: torch.Tensor,\n",
        "                         labels: torch.Tensor,\n",
        "                         window_size: int = 100) -> dict:\n",
        "        \"\"\"Analyze how feature importance changes over time\"\"\"\n",
        "        n_windows = len(features) // window_size\n",
        "        temporal_results = {group: [] for group in self.feature_groups}\n",
        "\n",
        "        for i in range(n_windows):\n",
        "            start_idx = i * window_size\n",
        "            end_idx = start_idx + window_size\n",
        "\n",
        "            window_results = self.analyze_feature_importance(\n",
        "                features[start_idx:end_idx],\n",
        "                labels[start_idx:end_idx]\n",
        "            )\n",
        "\n",
        "            for group, metrics in window_results.items():\n",
        "                temporal_results[group].append(metrics['importance'])\n",
        "\n",
        "        return temporal_results\n",
        "\n",
        "# Training utilities\n",
        "def train_real_world_model(model: nn.Module,\n",
        "                          features: torch.Tensor,\n",
        "                          labels: torch.Tensor,\n",
        "                          val_split: float = 0.2) -> Tuple[List[float], List[float]]:\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Split data\n",
        "    split_idx = int(len(features) * (1 - val_split))\n",
        "    train_features, val_features = features[:split_idx], features[split_idx:]\n",
        "    train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(50):\n",
        "        # Training\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_features)\n",
        "        loss = criterion(outputs, train_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(val_features)\n",
        "            val_loss = criterion(val_outputs, val_labels)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "YJo4fxzRCnqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapting our model for real-world datasets, focusing on a financial time series dataset, since it often contains both meaningful and spurious patterns."
      ],
      "metadata": {
        "id": "a0npe2RVCuLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_real_world_analysis(symbol: str = \"SPY\"):\n",
        "    # Process data\n",
        "    processor = FinancialDataProcessor()\n",
        "    features, labels = processor.create_features(symbol)\n",
        "\n",
        "    # Convert to tensors\n",
        "    features = torch.FloatTensor(features)\n",
        "    labels = torch.FloatTensor(labels)\n",
        "\n",
        "    # Create and train model\n",
        "    model = RealWorldTransformer(input_shape=features.shape[1:])\n",
        "    train_losses, val_losses = train_real_world_model(model, features, labels)\n",
        "\n",
        "    # Analyze results\n",
        "    analyzer = RealWorldAnalyzer(model)\n",
        "    importance_results = analyzer.analyze_feature_importance(features, labels)\n",
        "    temporal_results = analyzer.temporal_stability(features, labels)\n",
        "\n",
        "    return {\n",
        "        'importance': importance_results,\n",
        "        'temporal': temporal_results,\n",
        "        'training': {'train_loss': train_losses, 'val_loss': val_losses}\n",
        "    }\n",
        "\n",
        "# Run analysis\n",
        "results = run_real_world_analysis()\n",
        "\n",
        "print(\"\\nFeature Group Importance:\")\n",
        "for group, metrics in results['importance'].items():\n",
        "    print(f\"{group}: {metrics['importance']:.3f} (standalone acc: {metrics['standalone_acc']:.3f})\")\n",
        "\n",
        "print(\"\\nTemporal Stability:\")\n",
        "for group, values in results['temporal'].items():\n",
        "    stability = np.std(values)\n",
        "    print(f\"{group} stability (std): {stability:.3f}\")"
      ],
      "metadata": {
        "id": "9YmvTGeyC17M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the implementation"
      ],
      "metadata": {
        "id": "M2pQY0NoC4-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "class ResultsAnalyzer:\n",
        "    def __init__(self, results):\n",
        "        self.importance = results['importance']\n",
        "        self.temporal = results['temporal']\n",
        "        self.training = results['training']\n",
        "\n",
        "    def analyze_feature_dependencies(self):\n",
        "        technical_imp = np.array(self.temporal['technical'])\n",
        "        calendar_imp = np.array(self.temporal['calendar'])\n",
        "\n",
        "        correlation = np.corrcoef(technical_imp, calendar_imp)[0,1]\n",
        "        granger_result = self._granger_causality(technical_imp, calendar_imp)\n",
        "\n",
        "        return {\n",
        "            'correlation': correlation,\n",
        "            'granger_causality': granger_result\n",
        "        }\n",
        "\n",
        "    def _granger_causality(self, x, y, max_lag=5):\n",
        "        min_aic = np.inf\n",
        "        best_lag = 0\n",
        "\n",
        "        for lag in range(1, max_lag + 1):\n",
        "            aic = self._var_aic(x, y, lag)\n",
        "            if aic < min_aic:\n",
        "                min_aic = aic\n",
        "                best_lag = lag\n",
        "\n",
        "        return {'optimal_lag': best_lag, 'aic': min_aic}\n",
        "\n",
        "    def _var_aic(self, x, y, lag):\n",
        "        # Simple VAR model AIC calculation\n",
        "        n = len(x) - lag\n",
        "        X = np.column_stack([x[lag:], y[lag:]])\n",
        "        residuals = np.diff(X, axis=0)\n",
        "        sse = np.sum(residuals**2)\n",
        "        return np.log(sse/n) + 2 * lag/n\n",
        "\n",
        "    def analyze_temporal_patterns(self):\n",
        "        patterns = {}\n",
        "        for group, values in self.temporal.items():\n",
        "            values = np.array(values)\n",
        "            patterns[group] = {\n",
        "                'trend': np.polyfit(range(len(values)), values, 1)[0],\n",
        "                'seasonality': self._detect_seasonality(values),\n",
        "                'volatility': np.std(values)\n",
        "            }\n",
        "        return patterns\n",
        "\n",
        "    def _detect_seasonality(self, values, freq=10):\n",
        "        fft = np.fft.fft(values)\n",
        "        power = np.abs(fft)**2\n",
        "        frequencies = np.fft.fftfreq(len(values))\n",
        "        main_freq = frequencies[np.argmax(power[1:])]\n",
        "        return 1/main_freq if main_freq != 0 else 0\n",
        "\n",
        "    def analyze_learning_dynamics(self):\n",
        "        train_loss = np.array(self.training['train_loss'])\n",
        "        val_loss = np.array(self.training['val_loss'])\n",
        "\n",
        "        return {\n",
        "            'convergence_rate': self._calculate_convergence_rate(train_loss),\n",
        "            'generalization_gap': np.mean(val_loss - train_loss),\n",
        "            'stability': np.std(val_loss[-10:])  # Last 10 epochs\n",
        "        }\n",
        "\n",
        "    def _calculate_convergence_rate(self, loss):\n",
        "        # Fit exponential decay\n",
        "        x = np.arange(len(loss))\n",
        "        y = np.log(loss)\n",
        "        slope = np.polyfit(x, y, 1)[0]\n",
        "        return np.exp(slope)\n",
        "\n",
        "    def visualize_results(self):\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        # Feature importance over time\n",
        "        for group, values in self.temporal.items():\n",
        "            axes[0,0].plot(values, label=group)\n",
        "        axes[0,0].set_title('Feature Importance Over Time')\n",
        "        axes[0,0].legend()\n",
        "\n",
        "        # Training dynamics\n",
        "        axes[0,1].plot(self.training['train_loss'], label='Train')\n",
        "        axes[0,1].plot(self.training['val_loss'], label='Validation')\n",
        "        axes[0,1].set_title('Training Dynamics')\n",
        "        axes[0,1].legend()\n",
        "\n",
        "        # Feature correlations\n",
        "        axes[1,0].scatter(self.temporal['technical'],\n",
        "                         self.temporal['calendar'])\n",
        "        axes[1,0].set_title('Technical vs Calendar Features')\n",
        "\n",
        "        # Overall importance\n",
        "        importances = [m['importance'] for m in self.importance.values()]\n",
        "        axes[1,1].bar(self.importance.keys(), importances)\n",
        "        axes[1,1].set_title('Overall Feature Importance')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "# Run detailed analysis\n",
        "analyzer = ResultsAnalyzer(results)\n",
        "\n",
        "dependencies = analyzer.analyze_feature_dependencies()\n",
        "temporal_patterns = analyzer.analyze_temporal_patterns()\n",
        "learning_dynamics = analyzer.analyze_learning_dynamics()\n",
        "\n",
        "print(\"\\nFeature Dependencies:\")\n",
        "print(f\"Correlation: {dependencies['correlation']:.3f}\")\n",
        "print(f\"Optimal lag: {dependencies['granger_causality']['optimal_lag']}\")\n",
        "\n",
        "print(\"\\nTemporal Patterns:\")\n",
        "for group, metrics in temporal_patterns.items():\n",
        "    print(f\"\\n{group}:\")\n",
        "    print(f\"Trend: {metrics['trend']:.3f}\")\n",
        "    print(f\"Seasonality period: {metrics['seasonality']:.1f}\")\n",
        "    print(f\"Volatility: {metrics['volatility']:.3f}\")\n",
        "\n",
        "print(\"\\nLearning Dynamics:\")\n",
        "print(f\"Convergence rate: {learning_dynamics['convergence_rate']:.3f}\")\n",
        "print(f\"Generalization gap: {learning_dynamics['generalization_gap']:.3f}\")\n",
        "print(f\"Stability: {learning_dynamics['stability']:.3f}\")\n",
        "\n",
        "# Visualize results\n",
        "fig = analyzer.visualize_results()"
      ],
      "metadata": {
        "id": "jAS4va4UC7gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key findings from the analysis of the finacial data experiments reveals:\n",
        "\n",
        "Superposition Effects:\n",
        "*   Technical features show higher individual importance but exhibit significant overlap in representation\n",
        "*   Calendar effects demonstrate periodic interference with technical features\n",
        "*   Model learns to share capacity between feature types based on temporal relevance\n",
        "\n",
        "\n",
        "Feature Dependencies:\n",
        "\n",
        "\n",
        "* Non-linear interactions between technical and calendar features\n",
        "*   Temporal lag suggests causal relationships between feature groups\n",
        "*   Seasonality patterns in feature importance align with market regimes\n",
        "\n",
        "Model Dynamics:\n",
        "\n",
        "\n",
        "*   Convergence rate indicates efficient learning of genuine patterns\n",
        "\n",
        "*   Generalization gap reveals potential overreliance on spurious correlations\n",
        "\n",
        "*   Feature importance stability varies with market volatility\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WhCeohfmC-Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from scipy.stats import entropy\n",
        "\n",
        "class AdvancedMetricsAnalyzer:\n",
        "    def __init__(self, model, features, labels):\n",
        "        self.model = model\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.activations = {}\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                self.activations[name] = output.detach().cpu().numpy()\n",
        "            return hook\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                module.register_forward_hook(hook_fn(name))\n",
        "\n",
        "    def analyze_information_flow(self):\n",
        "        with torch.no_grad():\n",
        "            _ = self.model(self.features)\n",
        "\n",
        "        layer_info = {}\n",
        "        prev_layer = None\n",
        "\n",
        "        for name, acts in sorted(self.activations.items()):\n",
        "            # Flatten activations\n",
        "            acts_flat = acts.reshape(acts.shape[0], -1)\n",
        "\n",
        "            # Information content\n",
        "            info_content = self._calculate_information_content(acts_flat)\n",
        "\n",
        "            # Information flow from previous layer\n",
        "            info_flow = 0\n",
        "            if prev_layer is not None:\n",
        "                info_flow = mutual_info_score(\n",
        "                    acts_flat.mean(axis=1),\n",
        "                    prev_layer.mean(axis=1)\n",
        "                )\n",
        "\n",
        "            layer_info[name] = {\n",
        "                'info_content': info_content,\n",
        "                'info_flow': info_flow\n",
        "            }\n",
        "\n",
        "            prev_layer = acts_flat\n",
        "\n",
        "        return layer_info\n",
        "\n",
        "    def _calculate_information_content(self, activations):\n",
        "        # Use histogram to estimate probability distribution\n",
        "        hist, _ = np.histogramdd(activations, bins=20)\n",
        "        prob = hist / np.sum(hist)\n",
        "        return entropy(prob.flatten())\n",
        "\n",
        "    def analyze_feature_compression(self):\n",
        "        compression_metrics = {}\n",
        "\n",
        "        for name, acts in self.activations.items():\n",
        "            # Reshape activations\n",
        "            acts_flat = acts.reshape(acts.shape[0], -1)\n",
        "\n",
        "            # SVD analysis\n",
        "            U, S, Vh = np.linalg.svd(acts_flat, full_matrices=False)\n",
        "\n",
        "            # Calculate metrics\n",
        "            total_variance = np.sum(S**2)\n",
        "            explained_ratios = (S**2) / total_variance\n",
        "\n",
        "            compression_metrics[name] = {\n",
        "                'effective_rank': np.sum(explained_ratios > 0.01),\n",
        "                'compression_ratio': explained_ratios[0] / np.mean(explained_ratios),\n",
        "                'spectral_decay': np.polyfit(range(len(S)), np.log(S), 1)[0]\n",
        "            }\n",
        "\n",
        "        return compression_metrics\n",
        "\n",
        "    def analyze_robustness_metrics(self):\n",
        "        robustness = {}\n",
        "\n",
        "        # Feature importance stability\n",
        "        importance_stability = self._analyze_importance_stability()\n",
        "\n",
        "        # Decision boundary characteristics\n",
        "        boundary_metrics = self._analyze_decision_boundary()\n",
        "\n",
        "        # Prediction confidence analysis\n",
        "        confidence_metrics = self._analyze_prediction_confidence()\n",
        "\n",
        "        robustness.update({\n",
        "            'importance_stability': importance_stability,\n",
        "            'boundary_metrics': boundary_metrics,\n",
        "            'confidence': confidence_metrics\n",
        "        })\n",
        "\n",
        "        return robustness\n",
        "\n",
        "    def _analyze_importance_stability(self):\n",
        "        n_samples = len(self.features)\n",
        "        bootstrap_results = []\n",
        "\n",
        "        for _ in range(20):  # 20 bootstrap iterations\n",
        "            idx = np.random.choice(n_samples, n_samples)\n",
        "            bootstrap_features = self.features[idx]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                orig_pred = self.model(bootstrap_features)\n",
        "\n",
        "                # Analyze feature importance stability\n",
        "                importance_scores = []\n",
        "                for i in range(bootstrap_features.shape[-1]):\n",
        "                    perturbed = bootstrap_features.clone()\n",
        "                    perturbed[..., i] *= 1.1  # 10% perturbation\n",
        "                    new_pred = self.model(perturbed)\n",
        "                    importance = torch.mean(torch.abs(new_pred - orig_pred))\n",
        "                    importance_scores.append(importance.item())\n",
        "\n",
        "            bootstrap_results.append(importance_scores)\n",
        "\n",
        "        return np.std(bootstrap_results, axis=0)\n",
        "\n",
        "    def _analyze_decision_boundary(self):\n",
        "        with torch.no_grad():\n",
        "            # Generate points near decision boundary\n",
        "            predictions = self.model(self.features)\n",
        "            boundary_mask = torch.abs(predictions - 0.5) < 0.1\n",
        "            boundary_points = self.features[boundary_mask]\n",
        "\n",
        "            if len(boundary_points) > 0:\n",
        "                # Analyze local linearity\n",
        "                eps = 1e-4\n",
        "                perturbed = boundary_points + torch.randn_like(boundary_points) * eps\n",
        "                pred_diff = self.model(perturbed) - self.model(boundary_points)\n",
        "                local_linearity = torch.mean(torch.abs(pred_diff)) / eps\n",
        "\n",
        "                return {\n",
        "                    'boundary_width': torch.std(boundary_points).item(),\n",
        "                    'local_linearity': local_linearity.item()\n",
        "                }\n",
        "            return None\n",
        "\n",
        "    def _analyze_prediction_confidence(self):\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(self.features)\n",
        "\n",
        "        return {\n",
        "            'mean_confidence': torch.mean(torch.abs(predictions - 0.5)).item(),\n",
        "            'confidence_std': torch.std(torch.abs(predictions - 0.5)).item()\n",
        "        }\n",
        "\n",
        "# Run advanced analysis\n",
        "analyzer = AdvancedMetricsAnalyzer(model, features, labels)\n",
        "\n",
        "info_flow = analyzer.analyze_information_flow()\n",
        "compression = analyzer.analyze_feature_compression()\n",
        "robustness = analyzer.analyze_robustness_metrics()\n",
        "\n",
        "print(\"\\nInformation Flow Analysis:\")\n",
        "for layer, metrics in info_flow.items():\n",
        "    print(f\"{layer}:\")\n",
        "    print(f\"  Information Content: {metrics['info_content']:.3f}\")\n",
        "    print(f\"  Information Flow: {metrics['info_flow']:.3f}\")\n",
        "\n",
        "print(\"\\nFeature Compression Analysis:\")\n",
        "for layer, metrics in compression.items():\n",
        "    print(f\"{layer}:\")\n",
        "    print(f\"  Effective Rank: {metrics['effective_rank']}\")\n",
        "    print(f\"  Compression Ratio: {metrics['compression_ratio']:.3f}\")\n",
        "    print(f\"  Spectral Decay: {metrics['spectral_decay']:.3f}\")\n",
        "\n",
        "print(\"\\nRobustness Metrics:\")\n",
        "print(f\"Feature Importance Stability: {np.mean(robustness['importance_stability']):.3f}\")\n",
        "if robustness['boundary_metrics']:\n",
        "    print(f\"Decision Boundary Width: {robustness['boundary_metrics']['boundary_width']:.3f}\")\n",
        "    print(f\"Local Linearity: {robustness['boundary_metrics']['local_linearity']:.3f}\")\n",
        "print(f\"Mean Prediction Confidence: {robustness['confidence']['mean_confidence']:.3f}\")"
      ],
      "metadata": {
        "id": "1wbVJEDKD2c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key findings from advanced metrics analysis:\n",
        "\n",
        "\n",
        "*   Information compression increases in deeper layers\n",
        "\n",
        "*   Feature importance stability varies significantly across bootstrapped samples\n",
        "\n",
        "*   Decision boundary shows local linearity, suggesting robust generalization\n",
        "\n",
        "*   High compression ratios indicate efficient feature representation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GkdLUXVXD6aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_advanced_metrics(info_flow, compression, robustness):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # Information Flow\n",
        "    layers = list(info_flow.keys())\n",
        "    info_content = [m['info_content'] for m in info_flow.values()]\n",
        "    info_flow_vals = [m['info_flow'] for m in info_flow.values()]\n",
        "\n",
        "    axes[0,0].plot(info_content, 'b-', label='Information Content')\n",
        "    axes[0,0].plot(info_flow_vals, 'r--', label='Information Flow')\n",
        "    axes[0,0].set_xticks(range(len(layers)))\n",
        "    axes[0,0].set_xticklabels(layers, rotation=45)\n",
        "    axes[0,0].set_title('Information Analysis')\n",
        "    axes[0,0].legend()\n",
        "\n",
        "    # Compression Metrics\n",
        "    eff_ranks = [m['effective_rank'] for m in compression.values()]\n",
        "    comp_ratios = [m['compression_ratio'] for m in compression.values()]\n",
        "\n",
        "    ax2 = axes[0,1].twinx()\n",
        "    axes[0,1].bar(range(len(layers)), eff_ranks, color='b', alpha=0.5, label='Effective Rank')\n",
        "    ax2.plot(range(len(layers)), comp_ratios, 'r-', label='Compression Ratio')\n",
        "    axes[0,1].set_xticks(range(len(layers)))\n",
        "    axes[0,1].set_xticklabels(layers, rotation=45)\n",
        "    axes[0,1].set_title('Compression Analysis')\n",
        "    axes[0,1].legend(loc='upper left')\n",
        "    ax2.legend(loc='upper right')\n",
        "\n",
        "    # Feature Importance Stability\n",
        "    sns.histplot(robustness['importance_stability'], ax=axes[1,0])\n",
        "    axes[1,0].set_title('Feature Importance Stability')\n",
        "\n",
        "    # Prediction Confidence\n",
        "    if robustness['boundary_metrics']:\n",
        "        metrics = [\n",
        "            robustness['boundary_metrics']['boundary_width'],\n",
        "            robustness['boundary_metrics']['local_linearity'],\n",
        "            robustness['confidence']['mean_confidence']\n",
        "        ]\n",
        "        labels = ['Boundary Width', 'Local Linearity', 'Mean Confidence']\n",
        "        axes[1,1].bar(labels, metrics)\n",
        "        axes[1,1].set_title('Robustness Metrics')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "fig = visualize_advanced_metrics(info_flow, compression, robustness)"
      ],
      "metadata": {
        "id": "2KgsgA4VEI21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualising the advanced analysis"
      ],
      "metadata": {
        "id": "3CWK8eaFENbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "class HealthcareDataProcessor:\n",
        "    def __init__(self, lookback: int = 10):\n",
        "        self.lookback = lookback\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def process_ehr_data(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        # Clinical features (genuine)\n",
        "        clinical_features = [\n",
        "            'heart_rate', 'blood_pressure', 'temperature', 'respiratory_rate',\n",
        "            'oxygen_saturation', 'lab_values', 'medications'\n",
        "        ]\n",
        "\n",
        "        # Administrative features (potentially spurious)\n",
        "        admin_features = [\n",
        "            'admission_type', 'insurance_type', 'facility_type',\n",
        "            'admission_day', 'length_of_stay'\n",
        "        ]\n",
        "\n",
        "        # Create sequences\n",
        "        X, y = self._create_sequences(data, clinical_features + admin_features)\n",
        "        return X, y\n",
        "\n",
        "    def _create_sequences(self, data: pd.DataFrame, features: list) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        X, y = [], []\n",
        "        for i in range(self.lookback, len(data)):\n",
        "            X.append(data[features].iloc[i-self.lookback:i].values)\n",
        "            y.append(data['outcome'].iloc[i])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "class HealthcareTransformer(torch.nn.Module):\n",
        "    def __init__(self, input_shape: Tuple[int, int], n_heads: int = 4):\n",
        "        super().__init__()\n",
        "        seq_len, n_features = input_shape\n",
        "\n",
        "        self.feature_embedding = torch.nn.Linear(n_features, 128)\n",
        "        self.pos_embedding = torch.nn.Parameter(torch.randn(1, seq_len, 128))\n",
        "\n",
        "        self.clinical_attention = torch.nn.MultiheadAttention(\n",
        "            embed_dim=128,\n",
        "            num_heads=n_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.admin_attention = torch.nn.MultiheadAttention(\n",
        "            embed_dim=128,\n",
        "            num_heads=n_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer = torch.nn.TransformerEncoder(\n",
        "            torch.nn.TransformerEncoderLayer(\n",
        "                d_model=128,\n",
        "                nhead=n_heads,\n",
        "                dim_feedforward=512,\n",
        "                batch_first=True\n",
        "            ),\n",
        "            num_layers=3\n",
        "        )\n",
        "\n",
        "        self.output = torch.nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.feature_embedding(x)\n",
        "        x = x + self.pos_embedding\n",
        "\n",
        "        # Separate attention mechanisms\n",
        "        clinical_out, _ = self.clinical_attention(x, x, x)\n",
        "        admin_out, _ = self.admin_attention(x, x, x)\n",
        "\n",
        "        # Combine attention outputs\n",
        "        x = clinical_out + admin_out\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return torch.sigmoid(self.output(x)).squeeze(-1)\n",
        "\n",
        "class HealthcareAnalyzer:\n",
        "    def __init__(self, model: torch.nn.Module):\n",
        "        self.model = model\n",
        "        self.feature_groups = {\n",
        "            'clinical': slice(0, 7),    # Clinical features\n",
        "            'admin': slice(7, 12)       # Administrative features\n",
        "        }\n",
        "\n",
        "    def analyze_attention_patterns(self,\n",
        "                                 features: torch.Tensor) -> Dict[str, np.ndarray]:\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            clinical_attention = self.model.clinical_attention(\n",
        "                features, features, features\n",
        "            )[1].numpy()\n",
        "\n",
        "            admin_attention = self.model.admin_attention(\n",
        "                features, features, features\n",
        "            )[1].numpy()\n",
        "\n",
        "        return {\n",
        "            'clinical': clinical_attention,\n",
        "            'admin': admin_attention\n",
        "        }\n",
        "\n",
        "    def analyze_feature_correlations(self,\n",
        "                                   features: torch.Tensor,\n",
        "                                   labels: torch.Tensor) -> Dict[str, float]:\n",
        "        correlations = {}\n",
        "        for group_name, group_slice in self.feature_groups.items():\n",
        "            group_features = features[:, :, group_slice]\n",
        "            correlation = np.corrcoef(\n",
        "                group_features.reshape(-1, group_features.shape[-1]).T,\n",
        "                labels.numpy().reshape(-1, 1).T\n",
        "            )[:-1, -1]\n",
        "            correlations[group_name] = np.mean(np.abs(correlation))\n",
        "        return correlations\n",
        "\n",
        "    def analyze_spurious_patterns(self,\n",
        "                                features: torch.Tensor,\n",
        "                                labels: torch.Tensor) -> Dict[str, float]:\n",
        "        results = {}\n",
        "\n",
        "        # Analyze temporal patterns\n",
        "        temporal_correlation = self._analyze_temporal_patterns(features, labels)\n",
        "\n",
        "        # Analyze demographic biases\n",
        "        demographic_bias = self._analyze_demographic_bias(features, labels)\n",
        "\n",
        "        results.update({\n",
        "            'temporal_correlation': temporal_correlation,\n",
        "            'demographic_bias': demographic_bias\n",
        "        })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _analyze_temporal_patterns(self,\n",
        "                                 features: torch.Tensor,\n",
        "                                 labels: torch.Tensor) -> float:\n",
        "        # Analyze correlation with time-based features\n",
        "        admin_features = features[:, :, self.feature_groups['admin']]\n",
        "        temporal_corr = np.corrcoef(\n",
        "            admin_features[:, :, -2].mean(axis=1),  # admission_day\n",
        "            labels.numpy()\n",
        "        )[0, 1]\n",
        "        return np.abs(temporal_corr)\n",
        "\n",
        "    def _analyze_demographic_bias(self,\n",
        "                                features: torch.Tensor,\n",
        "                                labels: torch.Tensor) -> float:\n",
        "        # Analyze correlation with administrative features\n",
        "        admin_features = features[:, :, self.feature_groups['admin']]\n",
        "        bias_score = np.abs(np.corrcoef(\n",
        "            admin_features[:, :, 1].mean(axis=1),  # insurance_type\n",
        "            labels.numpy()\n",
        "        )[0, 1])\n",
        "        return bias_score\n",
        "\n",
        "def train_healthcare_model(model: torch.nn.Module,\n",
        "                         features: torch.Tensor,\n",
        "                         labels: torch.Tensor,\n",
        "                         n_epochs: int = 50) -> Dict[str, list]:\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "    metrics = {\n",
        "        'train_loss': [],\n",
        "        'clinical_importance': [],\n",
        "        'admin_importance': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Track feature importance\n",
        "        clinical_grad = torch.autograd.grad(\n",
        "            loss,\n",
        "            model.clinical_attention.parameters(),\n",
        "            retain_graph=True\n",
        "        )[0].norm().item()\n",
        "\n",
        "        admin_grad = torch.autograd.grad(\n",
        "            loss,\n",
        "            model.admin_attention.parameters(),\n",
        "            retain_graph=True\n",
        "        )[0].norm().item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        metrics['train_loss'].append(loss.item())\n",
        "        metrics['clinical_importance'].append(clinical_grad)\n",
        "        metrics['admin_importance'].append(admin_grad)\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "SuWrL2cqERmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key findings of a test with synthetic healthcare data:\n",
        "\n",
        "\n",
        "*   Clinical features show stronger predictive power but exhibit superposition\n",
        "\n",
        "*   Administrative features reveal spurious correlations with outcomes\n",
        "\n",
        "*   Model learns to separate clinical and administrative attention patterns\n",
        "\n",
        "*   Temporal and demographic biases detected in predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "km1HRocMEYpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "class DetailedHealthcareAnalyzer:\n",
        "    def __init__(self, model, features, labels):\n",
        "        self.model = model\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def analyze_attention_bias(self):\n",
        "        \"\"\"Analyze bias in attention weights\"\"\"\n",
        "        clinical_attention = self.model.clinical_attention(\n",
        "            self.features, self.features, self.features\n",
        "        )[1].detach()\n",
        "\n",
        "        admin_attention = self.model.admin_attention(\n",
        "            self.features, self.features, self.features\n",
        "        )[1].detach()\n",
        "\n",
        "        # Calculate attention entropy\n",
        "        clinical_entropy = self._calculate_attention_entropy(clinical_attention)\n",
        "        admin_entropy = self._calculate_attention_entropy(admin_attention)\n",
        "\n",
        "        # Calculate attention bias towards specific features\n",
        "        clinical_bias = torch.std(clinical_attention.mean(dim=1), dim=1)\n",
        "        admin_bias = torch.std(admin_attention.mean(dim=1), dim=1)\n",
        "\n",
        "        return {\n",
        "            'clinical_entropy': clinical_entropy.mean().item(),\n",
        "            'admin_entropy': admin_entropy.mean().item(),\n",
        "            'clinical_bias': clinical_bias.mean().item(),\n",
        "            'admin_bias': admin_bias.mean().item()\n",
        "        }\n",
        "\n",
        "    def _calculate_attention_entropy(self, attention_weights):\n",
        "        # Normalize attention weights\n",
        "        attention_probs = torch.softmax(attention_weights, dim=-1)\n",
        "        entropy = -torch.sum(\n",
        "            attention_probs * torch.log(attention_probs + 1e-10),\n",
        "            dim=-1\n",
        "        )\n",
        "        return entropy\n",
        "\n",
        "    def analyze_feature_interactions(self):\n",
        "        \"\"\"Analyze interactions between clinical and administrative features\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            base_pred = self.model(self.features)\n",
        "\n",
        "            # Zero out clinical features\n",
        "            clinical_masked = self.features.clone()\n",
        "            clinical_masked[:, :, :7] = 0\n",
        "            clinical_only_pred = self.model(clinical_masked)\n",
        "\n",
        "            # Zero out administrative features\n",
        "            admin_masked = self.features.clone()\n",
        "            admin_masked[:, :, 7:] = 0\n",
        "            admin_only_pred = self.model(admin_masked)\n",
        "\n",
        "            # Calculate interaction effects\n",
        "            interaction_effect = base_pred - (clinical_only_pred + admin_only_pred)\n",
        "\n",
        "        return {\n",
        "            'mean_interaction': interaction_effect.mean().item(),\n",
        "            'std_interaction': interaction_effect.std().item(),\n",
        "            'max_interaction': interaction_effect.max().item()\n",
        "        }\n",
        "\n",
        "    def analyze_temporal_stability(self, window_size=100):\n",
        "        \"\"\"Analyze prediction stability over time\"\"\"\n",
        "        predictions = []\n",
        "        importances = []\n",
        "\n",
        "        for i in range(0, len(self.features), window_size):\n",
        "            window_features = self.features[i:i+window_size]\n",
        "\n",
        "            # Get predictions\n",
        "            with torch.no_grad():\n",
        "                pred = self.model(window_features)\n",
        "                predictions.append(pred.mean().item())\n",
        "\n",
        "            # Calculate feature importance\n",
        "            importance = self._calculate_feature_importance(window_features)\n",
        "            importances.append(importance)\n",
        "\n",
        "        return {\n",
        "            'prediction_stability': np.std(predictions),\n",
        "            'importance_stability': np.std(importances, axis=0),\n",
        "            'temporal_correlation': np.corrcoef(predictions)[0,1]\n",
        "        }\n",
        "\n",
        "    def _calculate_feature_importance(self, features):\n",
        "        importance = []\n",
        "        base_pred = self.model(features)\n",
        "\n",
        "        for i in range(features.shape[2]):\n",
        "            perturbed = features.clone()\n",
        "            perturbed[:, :, i] *= 1.1\n",
        "            new_pred = self.model(perturbed)\n",
        "            importance.append((new_pred - base_pred).abs().mean().item())\n",
        "\n",
        "        return importance\n",
        "\n",
        "    def analyze_outcome_bias(self):\n",
        "        \"\"\"Analyze bias in outcome predictions\"\"\"\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(self.features)\n",
        "\n",
        "        # Calculate performance metrics for different groups\n",
        "        admin_features = self.features[:, :, 7:]\n",
        "\n",
        "        # Analyze bias across insurance types\n",
        "        insurance_types = admin_features[:, 0, 1].long()\n",
        "        insurance_metrics = {}\n",
        "\n",
        "        for ins_type in torch.unique(insurance_types):\n",
        "            mask = insurance_types == ins_type\n",
        "            if mask.sum() > 0:\n",
        "                group_preds = predictions[mask]\n",
        "                group_labels = self.labels[mask]\n",
        "\n",
        "                fpr, tpr, _ = roc_curve(group_labels.numpy(), group_preds.numpy())\n",
        "                group_auc = auc(fpr, tpr)\n",
        "\n",
        "                insurance_metrics[f'type_{ins_type.item()}'] = {\n",
        "                    'auc': group_auc,\n",
        "                    'mean_pred': group_preds.mean().item(),\n",
        "                    'true_rate': group_labels.mean().item()\n",
        "                }\n",
        "\n",
        "        return insurance_metrics\n",
        "\n",
        "# Run detailed analysis\n",
        "analyzer = DetailedHealthcareAnalyzer(model, features_tensor, labels_tensor)\n",
        "\n",
        "attention_bias = analyzer.analyze_attention_bias()\n",
        "feature_interactions = analyzer.analyze_feature_interactions()\n",
        "temporal_stability = analyzer.analyze_temporal_stability()\n",
        "outcome_bias = analyzer.analyze_outcome_bias()\n",
        "\n",
        "print(\"\\nAttention Bias Analysis:\")\n",
        "for metric, value in attention_bias.items():\n",
        "    print(f\"{metric}: {value:.3f}\")\n",
        "\n",
        "print(\"\\nFeature Interactions:\")\n",
        "for metric, value in feature_interactions.items():\n",
        "    print(f\"{metric}: {value:.3f}\")\n",
        "\n",
        "print(\"\\nTemporal Stability:\")\n",
        "print(f\"Prediction stability: {temporal_stability['prediction_stability']:.3f}\")\n",
        "print(f\"Mean importance stability: {temporal_stability['importance_stability'].mean():.3f}\")\n",
        "\n",
        "print(\"\\nOutcome Bias Analysis:\")\n",
        "for ins_type, metrics in outcome_bias.items():\n",
        "    print(f\"\\n{ins_type}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.3f}\")"
      ],
      "metadata": {
        "id": "V23rB45mErlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A more detailed analysis reveals:\n",
        "\n",
        "*   Attention mechanisms show higher entropy for clinical features, indicating more distributed information processing\n",
        "\n",
        "*  Feature interactions reveal significant coupling between clinical and administrative features, suggesting potential confounding\n",
        "\n",
        "*   Temporal stability varies by insurance type, indicating demographic bias in model predictions\n",
        "*   Clinical features show more stable importance scores over time compared to administrative features\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3C2_q01tEu8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from scipy.stats import entropy\n",
        "\n",
        "class AutomatedDetector:\n",
        "    def __init__(self, model: torch.nn.Module, threshold: float = 0.7):\n",
        "        self.model = model\n",
        "        self.threshold = threshold\n",
        "        self.hooks = []\n",
        "        self.activations = {}\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                self.activations[name] = output.detach()\n",
        "            return hook\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, (torch.nn.Linear, torch.nn.MultiheadAttention)):\n",
        "                self.hooks.append(module.register_forward_hook(hook_fn(name)))\n",
        "\n",
        "    def detect_spurious_correlations(self,\n",
        "                                   features: torch.Tensor,\n",
        "                                   labels: torch.Tensor,\n",
        "                                   feature_groups: Dict[str, slice]) -> Dict[str, float]:\n",
        "        \"\"\"Detect spurious correlations in feature groups\"\"\"\n",
        "        scores = {}\n",
        "\n",
        "        for group_name, group_slice in feature_groups.items():\n",
        "            # Extract group features\n",
        "            group_features = features[:, :, group_slice]\n",
        "\n",
        "            # Calculate predictive power\n",
        "            pred_power = self._calculate_predictive_power(group_features, labels)\n",
        "\n",
        "            # Calculate stability\n",
        "            stability = self._calculate_stability(group_features, labels)\n",
        "\n",
        "            # Calculate independence\n",
        "            independence = self._calculate_feature_independence(group_features)\n",
        "\n",
        "            # Combine metrics\n",
        "            spurious_score = pred_power * (1 - stability) * (1 - independence)\n",
        "            scores[group_name] = spurious_score.item()\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def detect_superposition(self, features: torch.Tensor) -> Dict[str, float]:\n",
        "        \"\"\"Detect superposition in model layers\"\"\"\n",
        "        superposition_scores = {}\n",
        "\n",
        "        # Forward pass to collect activations\n",
        "        with torch.no_grad():\n",
        "            _ = self.model(features)\n",
        "\n",
        "        for name, activations in self.activations.items():\n",
        "            # Reshape activations\n",
        "            acts = activations.reshape(-1, activations.shape[-1])\n",
        "\n",
        "            # Calculate alignment score\n",
        "            alignment = self._calculate_alignment(acts)\n",
        "\n",
        "            # Calculate interference\n",
        "            interference = self._calculate_interference(acts)\n",
        "\n",
        "            # Combine metrics\n",
        "            superposition_score = alignment * interference\n",
        "            superposition_scores[name] = superposition_score.item()\n",
        "\n",
        "        return superposition_scores\n",
        "\n",
        "    def _calculate_predictive_power(self, features: torch.Tensor, labels: torch.Tensor) -> float:\n",
        "        with torch.no_grad():\n",
        "            # Use mutual information as measure of predictive power\n",
        "            flat_features = features.reshape(-1, features.shape[-1])\n",
        "            mi_scores = []\n",
        "\n",
        "            for i in range(flat_features.shape[1]):\n",
        "                mi = mutual_info_score(\n",
        "                    flat_features[:, i].numpy(),\n",
        "                    labels.numpy()\n",
        "                )\n",
        "                mi_scores.append(mi)\n",
        "\n",
        "        return np.mean(mi_scores)\n",
        "\n",
        "    def _calculate_stability(self, features: torch.Tensor, labels: torch.Tensor) -> float:\n",
        "        # Calculate stability across bootstrap samples\n",
        "        n_bootstrap = 20\n",
        "        predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(n_bootstrap):\n",
        "                idx = torch.randint(len(features), (len(features),))\n",
        "                bootstrap_features = features[idx]\n",
        "                bootstrap_labels = labels[idx]\n",
        "\n",
        "                pred = self.model(bootstrap_features)\n",
        "                predictions.append(pred.numpy())\n",
        "\n",
        "        return np.mean([np.corrcoef(p1, p2)[0,1]\n",
        "                       for p1 in predictions\n",
        "                       for p2 in predictions])\n",
        "\n",
        "    def _calculate_feature_independence(self, features: torch.Tensor) -> float:\n",
        "        # Calculate feature correlations\n",
        "        flat_features = features.reshape(-1, features.shape[-1])\n",
        "        corr_matrix = np.corrcoef(flat_features.T)\n",
        "        return 1 - np.mean(np.abs(corr_matrix - np.eye(corr_matrix.shape[0])))\n",
        "\n",
        "    def _calculate_alignment(self, activations: torch.Tensor) -> float:\n",
        "        # SVD analysis\n",
        "        U, S, V = torch.svd(activations)\n",
        "        singular_values = S.numpy()\n",
        "\n",
        "        # Calculate alignment using singular value decay\n",
        "        sv_ratios = singular_values[1:] / singular_values[:-1]\n",
        "        return float(np.mean(sv_ratios))\n",
        "\n",
        "    def _calculate_interference(self, activations: torch.Tensor) -> float:\n",
        "        # Calculate interference using activation statistics\n",
        "        correlation = torch.corrcoef(activations.T)\n",
        "        return float(torch.mean(torch.abs(correlation - torch.eye(correlation.shape[0]))))\n",
        "\n",
        "class AutomatedMitigator:\n",
        "    def __init__(self, model: torch.nn.Module, detector: AutomatedDetector):\n",
        "        self.model = model\n",
        "        self.detector = detector\n",
        "\n",
        "    def mitigate_spurious_correlations(self,\n",
        "                                     features: torch.Tensor,\n",
        "                                     labels: torch.Tensor,\n",
        "                                     feature_groups: Dict[str, slice]) -> None:\n",
        "        \"\"\"Apply mitigation strategies for spurious correlations\"\"\"\n",
        "        scores = self.detector.detect_spurious_correlations(\n",
        "            features, labels, feature_groups\n",
        "        )\n",
        "\n",
        "        for group_name, score in scores.items():\n",
        "            if score > self.detector.threshold:\n",
        "                print(f\"Mitigating spurious correlation in {group_name}\")\n",
        "                self._apply_regularization(feature_groups[group_name])\n",
        "\n",
        "    def mitigate_superposition(self,\n",
        "                             features: torch.Tensor) -> None:\n",
        "        \"\"\"Apply mitigation strategies for superposition\"\"\"\n",
        "        scores = self.detector.detect_superposition(features)\n",
        "\n",
        "        for layer_name, score in scores.items():\n",
        "            if score > self.detector.threshold:\n",
        "                print(f\"Mitigating superposition in {layer_name}\")\n",
        "                self._apply_orthogonality_constraint(layer_name)\n",
        "\n",
        "    def _apply_regularization(self, feature_slice: slice):\n",
        "        \"\"\"Apply regularization to reduce spurious correlations\"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                # Add L1 regularization to weights corresponding to spurious features\n",
        "                param.data[..., feature_slice] *= 0.9\n",
        "\n",
        "    def _apply_orthogonality_constraint(self, layer_name: str):\n",
        "        \"\"\"Apply orthogonality constraint to reduce superposition\"\"\"\n",
        "        for name, module in self.model.named_modules():\n",
        "            if name == layer_name and hasattr(module, 'weight'):\n",
        "                W = module.weight.data\n",
        "                U, _, V = torch.svd(W)\n",
        "                # Update weights to be more orthogonal\n",
        "                module.weight.data = torch.mm(U, V.t())\n",
        "\n",
        "# Test the system\n",
        "def test_detection_system(model, features, labels, feature_groups):\n",
        "    detector = AutomatedDetector(model)\n",
        "    mitigator = AutomatedMitigator(model, detector)\n",
        "\n",
        "    # Initial detection\n",
        "    spurious_scores = detector.detect_spurious_correlations(features, labels, feature_groups)\n",
        "    superposition_scores = detector.detect_superposition(features)\n",
        "\n",
        "    print(\"\\nInitial Detection:\")\n",
        "    print(\"Spurious Correlation Scores:\", spurious_scores)\n",
        "    print(\"Superposition Scores:\", superposition_scores)\n",
        "\n",
        "    # Apply mitigation\n",
        "    mitigator.mitigate_spurious_correlations(features, labels, feature_groups)\n",
        "    mitigator.mitigate_superposition(features)\n",
        "\n",
        "    # Post-mitigation detection\n",
        "    new_spurious_scores = detector.detect_spurious_correlations(features, labels, feature_groups)\n",
        "    new_superposition_scores = detector.detect_superposition(features)\n",
        "\n",
        "    print(\"\\nPost-Mitigation Detection:\")\n",
        "    print(\"Spurious Correlation Scores:\", new_spurious_scores)\n",
        "    print(\"Superposition Scores:\", new_superposition_scores)"
      ],
      "metadata": {
        "id": "i0qF6e9zFDCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An automated detection system that performs:\n",
        "\n",
        "\n",
        "*   Automated detection using predictive power, stability, and independence metrics\n",
        "\n",
        "*  Mitigation through regularization and orthogonality constraints\n",
        "\n",
        "*   Real-time monitoring and adaptation\n",
        "\n",
        "*  Performance impact assessment\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MnMWRSbTFJbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from healthcare_analysis import HealthcareTransformer, generate_synthetic_ehr, HealthcareDataProcessor\n",
        "\n",
        "# Generate test data\n",
        "data = generate_synthetic_ehr()\n",
        "processor = HealthcareDataProcessor()\n",
        "features, labels = processor.process_ehr_data(data)\n",
        "\n",
        "# Setup model and feature groups\n",
        "features_tensor = torch.FloatTensor(features)\n",
        "labels_tensor = torch.FloatTensor(labels)\n",
        "model = HealthcareTransformer(input_shape=features.shape[1:])\n",
        "\n",
        "feature_groups = {\n",
        "    'clinical': slice(0, 7),\n",
        "    'administrative': slice(7, 12)\n",
        "}\n",
        "\n",
        "# Test detection and mitigation\n",
        "test_detection_system(model, features_tensor, labels_tensor, feature_groups)\n",
        "\n",
        "# Evaluate model performance before and after mitigation\n",
        "def evaluate_performance(features, labels):\n",
        "    with torch.no_grad():\n",
        "        predictions = model(features)\n",
        "        accuracy = ((predictions > 0.5) == labels).float().mean()\n",
        "    return accuracy.item()\n",
        "\n",
        "initial_accuracy = evaluate_performance(features_tensor, labels_tensor)\n",
        "print(f\"\\nInitial Accuracy: {initial_accuracy:.3f}\")\n",
        "\n",
        "# Apply mitigation\n",
        "detector = AutomatedDetector(model)\n",
        "mitigator = AutomatedMitigator(model, detector)\n",
        "mitigator.mitigate_spurious_correlations(features_tensor, labels_tensor, feature_groups)\n",
        "mitigator.mitigate_superposition(features_tensor)\n",
        "\n",
        "final_accuracy = evaluate_performance(features_tensor, labels_tensor)\n",
        "print(f\"Final Accuracy: {final_accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "LAphhSW7Faxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the automated system"
      ],
      "metadata": {
        "id": "NBo732NDFgb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from scipy.stats import wasserstein_distance\n",
        "from sklearn.decomposition import FastICA\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "class EnhancedDetector(AutomatedDetector):\n",
        "    def __init__(self, model: torch.nn.Module, threshold: float = 0.7):\n",
        "        super().__init__(model, threshold)\n",
        "\n",
        "    def detect_spurious_correlations(self,\n",
        "                                   features: torch.Tensor,\n",
        "                                   labels: torch.Tensor,\n",
        "                                   feature_groups: Dict[str, slice]) -> Dict[str, Dict[str, float]]:\n",
        "        scores = {}\n",
        "\n",
        "        for group_name, group_slice in feature_groups.items():\n",
        "            group_features = features[:, :, group_slice]\n",
        "\n",
        "            metrics = {\n",
        "                'counterfactual_impact': self._measure_counterfactual_impact(\n",
        "                    group_features, features, labels\n",
        "                ),\n",
        "                'distribution_shift': self._measure_distribution_shift(\n",
        "                    group_features, labels\n",
        "                ),\n",
        "                'temporal_consistency': self._measure_temporal_consistency(\n",
        "                    group_features, labels\n",
        "                ),\n",
        "                'causal_strength': self._measure_causal_strength(\n",
        "                    group_features, features, labels\n",
        "                )\n",
        "            }\n",
        "\n",
        "            scores[group_name] = metrics\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def _measure_counterfactual_impact(self,\n",
        "                                     group_features: torch.Tensor,\n",
        "                                     full_features: torch.Tensor,\n",
        "                                     labels: torch.Tensor) -> float:\n",
        "        with torch.no_grad():\n",
        "            # Original predictions\n",
        "            orig_pred = self.model(full_features)\n",
        "\n",
        "            # Counterfactual predictions (permuted group features)\n",
        "            cf_features = full_features.clone()\n",
        "            permuted_idx = torch.randperm(len(group_features))\n",
        "            cf_features[:, :, group_features.shape[2]:] = group_features[permuted_idx]\n",
        "            cf_pred = self.model(cf_features)\n",
        "\n",
        "            # Impact score\n",
        "            impact = torch.mean(torch.abs(orig_pred - cf_pred))\n",
        "\n",
        "        return impact.item()\n",
        "\n",
        "    def _measure_distribution_shift(self,\n",
        "                                  group_features: torch.Tensor,\n",
        "                                  labels: torch.Tensor) -> float:\n",
        "        # Split data into positive and negative classes\n",
        "        pos_features = group_features[labels == 1]\n",
        "        neg_features = group_features[labels == 0]\n",
        "\n",
        "        # Calculate Wasserstein distance between distributions\n",
        "        distances = []\n",
        "        for i in range(group_features.shape[2]):\n",
        "            dist = wasserstein_distance(\n",
        "                pos_features[:, 0, i].numpy(),\n",
        "                neg_features[:, 0, i].numpy()\n",
        "            )\n",
        "            distances.append(dist)\n",
        "\n",
        "        return np.mean(distances)\n",
        "\n",
        "    def _measure_temporal_consistency(self,\n",
        "                                    group_features: torch.Tensor,\n",
        "                                    labels: torch.Tensor,\n",
        "                                    window_size: int = 100) -> float:\n",
        "        consistencies = []\n",
        "\n",
        "        for i in range(0, len(group_features) - window_size, window_size):\n",
        "            window1 = group_features[i:i+window_size]\n",
        "            window2 = group_features[i+window_size:i+2*window_size]\n",
        "\n",
        "            if len(window2) == window_size:\n",
        "                consistency = torch.corrcoef(\n",
        "                    torch.cat([window1.mean(1), window2.mean(1)], dim=0)\n",
        "                )[0,1]\n",
        "                consistencies.append(consistency.item())\n",
        "\n",
        "        return np.mean(consistencies)\n",
        "\n",
        "    def _measure_causal_strength(self,\n",
        "                               group_features: torch.Tensor,\n",
        "                               full_features: torch.Tensor,\n",
        "                               labels: torch.Tensor) -> float:\n",
        "        # Use ICA to measure causal strength\n",
        "        ica = FastICA(n_components=min(5, group_features.shape[2]))\n",
        "        group_components = ica.fit_transform(\n",
        "            group_features.reshape(-1, group_features.shape[2]).numpy()\n",
        "        )\n",
        "\n",
        "        # Measure predictive power of independent components\n",
        "        with torch.no_grad():\n",
        "            orig_pred = self.model(full_features)\n",
        "            causal_strengths = []\n",
        "\n",
        "            for comp in range(group_components.shape[1]):\n",
        "                correlation = np.corrcoef(\n",
        "                    group_components[:, comp],\n",
        "                    orig_pred.numpy()\n",
        "                )[0,1]\n",
        "                causal_strengths.append(abs(correlation))\n",
        "\n",
        "        return np.mean(causal_strengths)\n",
        "\n",
        "class EnhancedMitigator(AutomatedMitigator):\n",
        "    def __init__(self, model: torch.nn.Module, detector: EnhancedDetector):\n",
        "        super().__init__(model, detector)\n",
        "\n",
        "    def mitigate_spurious_correlations(self,\n",
        "                                     features: torch.Tensor,\n",
        "                                     labels: torch.Tensor,\n",
        "                                     feature_groups: Dict[str, slice]) -> None:\n",
        "        scores = self.detector.detect_spurious_correlations(\n",
        "            features, labels, feature_groups\n",
        "        )\n",
        "\n",
        "        for group_name, metrics in scores.items():\n",
        "            if any(v > self.detector.threshold for v in metrics.values()):\n",
        "                self._apply_targeted_mitigation(\n",
        "                    feature_groups[group_name],\n",
        "                    metrics\n",
        "                )\n",
        "\n",
        "    def _apply_targeted_mitigation(self,\n",
        "                                 feature_slice: slice,\n",
        "                                 metrics: Dict[str, float]) -> None:\n",
        "        # Apply different strategies based on metrics\n",
        "        if metrics['counterfactual_impact'] > self.detector.threshold:\n",
        "            self._apply_counterfactual_regularization(feature_slice)\n",
        "\n",
        "        if metrics['distribution_shift'] > self.detector.threshold:\n",
        "            self._apply_distribution_matching(feature_slice)\n",
        "\n",
        "        if metrics['temporal_consistency'] < 0.5:\n",
        "            self._apply_temporal_smoothing(feature_slice)\n",
        "\n",
        "    def _apply_counterfactual_regularization(self, feature_slice: slice):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                grad_mask = torch.ones_like(param.data)\n",
        "                grad_mask[..., feature_slice] *= 0.5\n",
        "                param.data *= grad_mask\n",
        "\n",
        "    def _apply_distribution_matching(self, feature_slice: slice):\n",
        "        # Add instance normalization for distribution matching\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                module.weight.data[..., feature_slice] = \\\n",
        "                    torch.nn.functional.instance_norm(\n",
        "                        module.weight.data[..., feature_slice].unsqueeze(0)\n",
        "                    ).squeeze(0)\n",
        "\n",
        "    def _apply_temporal_smoothing(self, feature_slice: slice):\n",
        "        # Add temporal smoothing through exponential moving average\n",
        "        ema = torch.nn.Parameter(torch.zeros(feature_slice.stop - feature_slice.start))\n",
        "        momentum = 0.9\n",
        "\n",
        "        def temporal_hook(module, input):\n",
        "            nonlocal ema\n",
        "            ema.data = momentum * ema.data + (1 - momentum) * input[0][..., feature_slice].mean(0)\n",
        "            input[0][..., feature_slice] = (\n",
        "                input[0][..., feature_slice] * 0.8 + ema * 0.2\n",
        "            )\n",
        "            return input\n",
        "\n",
        "        self.model.register_forward_pre_hook(temporal_hook)"
      ],
      "metadata": {
        "id": "E_KEcMESFidy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhancing the automated system allows for:\n",
        "\n",
        "\n",
        "*   Counterfactual impact analysis\n",
        "*   Distribution shift detection\n",
        "\n",
        "*   Temporal consistency checking\n",
        "\n",
        "*   Causal strength measurement\n",
        "\n",
        "*   Targeted mitigation strategies\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XmjEbnaMFlrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_enhanced_system(model, features, labels, feature_groups):\n",
        "    detector = EnhancedDetector(model)\n",
        "    mitigator = EnhancedMitigator(model, detector)\n",
        "\n",
        "    # Initial detection\n",
        "    initial_scores = detector.detect_spurious_correlations(\n",
        "        features, labels, feature_groups\n",
        "    )\n",
        "\n",
        "    print(\"\\nInitial Detection:\")\n",
        "    for group, metrics in initial_scores.items():\n",
        "        print(f\"\\n{group}:\")\n",
        "        for metric, score in metrics.items():\n",
        "            print(f\"  {metric}: {score:.3f}\")\n",
        "\n",
        "    # Apply mitigation\n",
        "    mitigator.mitigate_spurious_correlations(features, labels, feature_groups)\n",
        "\n",
        "    # Post-mitigation detection\n",
        "    final_scores = detector.detect_spurious_correlations(\n",
        "        features, labels, feature_groups\n",
        "    )\n",
        "\n",
        "    print(\"\\nPost-Mitigation Detection:\")\n",
        "    for group, metrics in final_scores.items():\n",
        "        print(f\"\\n{group}:\")\n",
        "        for metric, score in metrics.items():\n",
        "            print(f\"  {metric}: {score:.3f}\")\n",
        "\n",
        "    # Performance impact\n",
        "    with torch.no_grad():\n",
        "        initial_pred = model(features)\n",
        "        initial_acc = ((initial_pred > 0.5) == labels).float().mean()\n",
        "\n",
        "        # Check generalization\n",
        "        permuted_idx = torch.randperm(len(features))\n",
        "        test_features = features[permuted_idx]\n",
        "        test_labels = labels[permuted_idx]\n",
        "\n",
        "        test_pred = model(test_features)\n",
        "        test_acc = ((test_pred > 0.5) == test_labels).float().mean()\n",
        "\n",
        "    return {\n",
        "        'initial_scores': initial_scores,\n",
        "        'final_scores': final_scores,\n",
        "        'accuracy': {\n",
        "            'initial': initial_acc.item(),\n",
        "            'generalization': test_acc.item()\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Run enhanced test\n",
        "results = test_enhanced_system(model, features_tensor, labels_tensor, feature_groups)\n",
        "\n",
        "print(\"\\nAccuracy Metrics:\")\n",
        "print(f\"Initial: {results['accuracy']['initial']:.3f}\")\n",
        "print(f\"Generalization: {results['accuracy']['generalization']:.3f}\")"
      ],
      "metadata": {
        "id": "JQ7c_W4uGJso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the enhanced system"
      ],
      "metadata": {
        "id": "FzT2rPETGMH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "class MitigationStrategies:\n",
        "    def __init__(self, model: nn.Module):\n",
        "        self.model = model\n",
        "\n",
        "    def adversarial_training(self,\n",
        "                           features: torch.Tensor,\n",
        "                           labels: torch.Tensor,\n",
        "                           feature_groups: Dict[str, slice],\n",
        "                           n_epochs: int = 10) -> nn.Module:\n",
        "        \"\"\"Adversarial training to reduce spurious correlations\"\"\"\n",
        "        optimizer = torch.optim.Adam(self.model.parameters())\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Generate adversarial examples\n",
        "            perturbed_features = features.clone().requires_grad_()\n",
        "            output = self.model(perturbed_features)\n",
        "            loss = criterion(output, 1 - labels)  # Flip labels\n",
        "            loss.backward()\n",
        "\n",
        "            # Create adversarial examples\n",
        "            with torch.no_grad():\n",
        "                for group_slice in feature_groups.values():\n",
        "                    perturbed_features.data[:, :, group_slice] += \\\n",
        "                        0.1 * torch.sign(perturbed_features.grad[:, :, group_slice])\n",
        "\n",
        "            # Train on both original and adversarial examples\n",
        "            optimizer.zero_grad()\n",
        "            orig_loss = criterion(self.model(features), labels)\n",
        "            adv_loss = criterion(self.model(perturbed_features), labels)\n",
        "            total_loss = 0.7 * orig_loss + 0.3 * adv_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def gradient_surgery(self,\n",
        "                        features: torch.Tensor,\n",
        "                        labels: torch.Tensor,\n",
        "                        feature_groups: Dict[str, slice]) -> nn.Module:\n",
        "        \"\"\"Apply gradient surgery to remove spurious correlations\"\"\"\n",
        "        optimizer = torch.optim.Adam(self.model.parameters())\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # Calculate group gradients\n",
        "        group_grads = {}\n",
        "        for group_name, group_slice in feature_groups.items():\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            group_grads[group_name] = {\n",
        "                name: param.grad.clone()\n",
        "                for name, param in self.model.named_parameters()\n",
        "                if param.grad is not None\n",
        "            }\n",
        "\n",
        "        # Project conflicting gradients\n",
        "        with torch.no_grad():\n",
        "            for param_name, param in self.model.named_parameters():\n",
        "                grads = [\n",
        "                    grads[param_name]\n",
        "                    for grads in group_grads.values()\n",
        "                    if param_name in grads\n",
        "                ]\n",
        "\n",
        "                if grads:\n",
        "                    # Project gradients to remove conflicts\n",
        "                    grad_tensor = torch.stack(grads)\n",
        "                    U, S, V = torch.svd(grad_tensor.view(len(grads), -1))\n",
        "                    projected_grad = V[0].view_as(param.grad)\n",
        "                    param.grad.copy_(projected_grad)\n",
        "\n",
        "        optimizer.step()\n",
        "        return self.model\n",
        "\n",
        "    def contrastive_regularization(self,\n",
        "                                 features: torch.Tensor,\n",
        "                                 labels: torch.Tensor,\n",
        "                                 feature_groups: Dict[str, slice],\n",
        "                                 temperature: float = 0.5) -> nn.Module:\n",
        "        \"\"\"Apply contrastive learning to separate genuine and spurious features\"\"\"\n",
        "        optimizer = torch.optim.Adam(self.model.parameters())\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # Extract embeddings for different feature groups\n",
        "        embeddings = {}\n",
        "        for group_name, group_slice in feature_groups.items():\n",
        "            group_features = features[:, :, group_slice]\n",
        "            embeddings[group_name] = self.model.feature_embedding(group_features)\n",
        "\n",
        "        # Contrastive loss between groups\n",
        "        contrast_loss = 0\n",
        "        for g1 in embeddings:\n",
        "            for g2 in embeddings:\n",
        "                if g1 != g2:\n",
        "                    similarity = torch.mm(\n",
        "                        embeddings[g1].view(-1, 128),\n",
        "                        embeddings[g2].view(-1, 128).t()\n",
        "                    )\n",
        "                    contrast_loss += torch.mean(\n",
        "                        torch.exp(similarity / temperature)\n",
        "                    )\n",
        "\n",
        "        # Combined loss\n",
        "        outputs = self.model(features)\n",
        "        pred_loss = criterion(outputs, labels)\n",
        "        total_loss = pred_loss + 0.1 * contrast_loss\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def uncertainty_weighting(self,\n",
        "                            features: torch.Tensor,\n",
        "                            labels: torch.Tensor,\n",
        "                            feature_groups: Dict[str, slice]) -> nn.Module:\n",
        "        \"\"\"Apply uncertainty-based feature weighting\"\"\"\n",
        "        optimizer = torch.optim.Adam(self.model.parameters())\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # Estimate uncertainty for each group\n",
        "        uncertainties = {}\n",
        "        for group_name, group_slice in feature_groups.items():\n",
        "            group_features = features[:, :, group_slice]\n",
        "\n",
        "            # Bootstrap uncertainty estimation\n",
        "            preds = []\n",
        "            for _ in range(10):\n",
        "                idx = torch.randint(len(features), (len(features),))\n",
        "                bootstrap_features = features[idx]\n",
        "                with torch.no_grad():\n",
        "                    pred = self.model(bootstrap_features)\n",
        "                    preds.append(pred)\n",
        "\n",
        "            uncertainty = torch.std(torch.stack(preds), dim=0)\n",
        "            uncertainties[group_name] = uncertainty\n",
        "\n",
        "        # Apply uncertainty weighting\n",
        "        weighted_features = features.clone()\n",
        "        for group_name, group_slice in feature_groups.items():\n",
        "            weight = 1 / (uncertainties[group_name] + 1e-5)\n",
        "            weighted_features[:, :, group_slice] *= weight.unsqueeze(-1)\n",
        "\n",
        "        # Train with weighted features\n",
        "        outputs = self.model(weighted_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return self.model"
      ],
      "metadata": {
        "id": "bV0GRvPXGOzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key findings of using alternative mitigation strategies:\n",
        "\n",
        "*   Adversarial training: Best for high-confidence spurious correlations\n",
        "\n",
        "*   Gradient surgery: Most effective for preserving task performance\n",
        "\n",
        "*   Contrastive regularization: Strong at feature disentanglemen\n",
        "*   Uncertainty weighting: Best for noisy or unstable features\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RxRutF9xGR__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mitigation_strategies(\n",
        "    features: torch.Tensor,\n",
        "    labels: torch.Tensor,\n",
        "    feature_groups: Dict[str, slice]\n",
        "):\n",
        "    strategies = MitigationStrategies(model)\n",
        "    detector = EnhancedDetector(model)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Test each strategy\n",
        "    for strategy in [\n",
        "        strategies.adversarial_training,\n",
        "        strategies.gradient_surgery,\n",
        "        strategies.contrastive_regularization,\n",
        "        strategies.uncertainty_weighting\n",
        "    ]:\n",
        "        strategy_name = strategy.__name__\n",
        "        print(f\"\\nTesting {strategy_name}\")\n",
        "\n",
        "        # Apply strategy\n",
        "        model_copy = copy.deepcopy(model)\n",
        "        strategy(features, labels, feature_groups)\n",
        "\n",
        "        # Evaluate\n",
        "        with torch.no_grad():\n",
        "            predictions = model_copy(features)\n",
        "            accuracy = ((predictions > 0.5) == labels).float().mean()\n",
        "\n",
        "            # Check spurious correlations\n",
        "            spurious_scores = detector.detect_spurious_correlations(\n",
        "                features, labels, feature_groups\n",
        "            )\n",
        "\n",
        "            # Test generalization\n",
        "            permuted_idx = torch.randperm(len(features))\n",
        "            test_acc = ((model_copy(features[permuted_idx]) > 0.5) ==\n",
        "                       labels[permuted_idx]).float().mean()\n",
        "\n",
        "        results[strategy_name] = {\n",
        "            'accuracy': accuracy.item(),\n",
        "            'generalization': test_acc.item(),\n",
        "            'spurious_scores': spurious_scores\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run evaluation\n",
        "results = evaluate_mitigation_strategies(features_tensor, labels_tensor, feature_groups)\n",
        "\n",
        "print(\"\\nStrategy Comparison:\")\n",
        "for strategy, metrics in results.items():\n",
        "    print(f\"\\n{strategy}:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.3f}\")\n",
        "    print(f\"Generalization: {metrics['generalization']:.3f}\")\n",
        "    for group, scores in metrics['spurious_scores'].items():\n",
        "        print(f\"{group} spurious correlation: {np.mean(list(scores.values())):.3f}\")"
      ],
      "metadata": {
        "id": "lv_vffH-GnRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the mitigation approaches"
      ],
      "metadata": {
        "id": "UJTiEFqTGo8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StrategyImpactAnalyzer:\n",
        "    def __init__(self, model, detector):\n",
        "        self.model = model\n",
        "        self.detector = detector\n",
        "\n",
        "    def analyze_feature_sensitivity(self, features, labels, feature_groups, strategy):\n",
        "        \"\"\"Analyze how strategy affects feature importance\"\"\"\n",
        "        original_importances = self._get_feature_importances(features, labels)\n",
        "\n",
        "        # Apply strategy\n",
        "        strategy(features, labels, feature_groups)\n",
        "        new_importances = self._get_feature_importances(features, labels)\n",
        "\n",
        "        return {\n",
        "            'importance_shift': {f: new_importances[f] - original_importances[f]\n",
        "                               for f in original_importances}\n",
        "        }\n",
        "\n",
        "    def _get_feature_importances(self, features, labels):\n",
        "        importances = {}\n",
        "        base_pred = self.model(features)\n",
        "\n",
        "        for i in range(features.shape[2]):\n",
        "            perturbed = features.clone()\n",
        "            perturbed[:, :, i] = 0\n",
        "            impact = torch.mean(torch.abs(self.model(perturbed) - base_pred))\n",
        "            importances[f'feature_{i}'] = impact.item()\n",
        "\n",
        "        return importances\n",
        "\n",
        "    def analyze_decision_boundary(self, features, labels, feature_groups, strategy):\n",
        "        \"\"\"Analyze decision boundary changes\"\"\"\n",
        "        # Original decision boundary\n",
        "        orig_boundary = self._get_decision_boundary(features, labels)\n",
        "\n",
        "        # Apply strategy\n",
        "        strategy(features, labels, feature_groups)\n",
        "        new_boundary = self._get_decision_boundary(features, labels)\n",
        "\n",
        "        return {\n",
        "            'boundary_shift': np.mean(np.abs(new_boundary - orig_boundary)),\n",
        "            'boundary_smoothness': self._measure_boundary_smoothness(features, labels)\n",
        "        }\n",
        "\n",
        "    def _get_decision_boundary(self, features, labels):\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(features)\n",
        "            return logits.numpy()\n",
        "\n",
        "    def _measure_boundary_smoothness(self, features, labels):\n",
        "        epsilon = 1e-4\n",
        "        perturbed = features + torch.randn_like(features) * epsilon\n",
        "\n",
        "        with torch.no_grad():\n",
        "            orig_pred = self.model(features)\n",
        "            pert_pred = self.model(perturbed)\n",
        "            smoothness = torch.mean(torch.abs(pert_pred - orig_pred)) / epsilon\n",
        "\n",
        "        return smoothness.item()\n",
        "\n",
        "    def analyze_representation_learning(self, features, labels, feature_groups, strategy):\n",
        "        \"\"\"Analyze changes in learned representations\"\"\"\n",
        "        # Get original representations\n",
        "        orig_repr = self._get_internal_representations(features)\n",
        "\n",
        "        # Apply strategy\n",
        "        strategy(features, labels, feature_groups)\n",
        "        new_repr = self._get_internal_representations(features)\n",
        "\n",
        "        return {\n",
        "            'representation_distance': self._measure_representation_distance(\n",
        "                orig_repr, new_repr\n",
        "            ),\n",
        "            'feature_disentanglement': self._measure_disentanglement(new_repr)\n",
        "        }\n",
        "\n",
        "    def _get_internal_representations(self, features):\n",
        "        representations = {}\n",
        "\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                representations[name] = output.detach()\n",
        "            return hook\n",
        "\n",
        "        hooks = []\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                hooks.append(module.register_forward_hook(hook_fn(name)))\n",
        "\n",
        "        self.model(features)\n",
        "\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        return representations\n",
        "\n",
        "    def _measure_representation_distance(self, repr1, repr2):\n",
        "        distances = {}\n",
        "        for name in repr1:\n",
        "            if name in repr2:\n",
        "                dist = torch.mean(torch.abs(repr1[name] - repr2[name]))\n",
        "                distances[name] = dist.item()\n",
        "        return distances\n",
        "\n",
        "    def _measure_disentanglement(self, representations):\n",
        "        disentanglement = {}\n",
        "        for name, repr_tensor in representations.items():\n",
        "            # Use correlation matrix\n",
        "            flat_repr = repr_tensor.reshape(-1, repr_tensor.shape[-1])\n",
        "            corr_matrix = torch.corrcoef(flat_repr.T)\n",
        "\n",
        "            # Measure off-diagonal correlations\n",
        "            disentanglement[name] = torch.mean(\n",
        "                torch.abs(corr_matrix - torch.eye(corr_matrix.shape[0]))\n",
        "            ).item()\n",
        "\n",
        "        return disentanglement\n",
        "\n",
        "def compare_strategy_impacts():\n",
        "    analyzer = StrategyImpactAnalyzer(model, detector)\n",
        "    strategies = MitigationStrategies(model)\n",
        "\n",
        "    strategy_impacts = {}\n",
        "\n",
        "    for strategy in [\n",
        "        strategies.adversarial_training,\n",
        "        strategies.gradient_surgery,\n",
        "        strategies.contrastive_regularization,\n",
        "        strategies.uncertainty_weighting\n",
        "    ]:\n",
        "        strategy_name = strategy.__name__\n",
        "        print(f\"\\nAnalyzing {strategy_name}\")\n",
        "\n",
        "        # Create fresh model copy\n",
        "        model_copy = copy.deepcopy(model)\n",
        "\n",
        "        # Analyze impacts\n",
        "        sensitivity = analyzer.analyze_feature_sensitivity(\n",
        "            features_tensor, labels_tensor, feature_groups, strategy\n",
        "        )\n",
        "\n",
        "        boundary = analyzer.analyze_decision_boundary(\n",
        "            features_tensor, labels_tensor, feature_groups, strategy\n",
        "        )\n",
        "\n",
        "        representation = analyzer.analyze_representation_learning(\n",
        "            features_tensor, labels_tensor, feature_groups, strategy\n",
        "        )\n",
        "\n",
        "        strategy_impacts[strategy_name] = {\n",
        "            'sensitivity': sensitivity,\n",
        "            'boundary': boundary,\n",
        "            'representation': representation\n",
        "        }\n",
        "\n",
        "    return strategy_impacts\n",
        "\n",
        "# Run analysis\n",
        "impacts = compare_strategy_impacts()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nStrategy Impact Summary:\")\n",
        "for strategy, metrics in impacts.items():\n",
        "    print(f\"\\n{strategy}:\")\n",
        "    print(f\"Feature Sensitivity Change: {np.mean(list(metrics['sensitivity']['importance_shift'].values())):.3f}\")\n",
        "    print(f\"Decision Boundary Shift: {metrics['boundary']['boundary_shift']:.3f}\")\n",
        "    print(f\"Average Disentanglement: {np.mean(list(metrics['representation']['feature_disentanglement'].values())):.3f}\")"
      ],
      "metadata": {
        "id": "EvyKZmJaGrwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A strategy impact analysis reveals:\n",
        "\n",
        "*   Adversarial training: Strongest boundary shifts, moderate feature disentanglement\n",
        "*   Gradient surgery: Best preservation of important features while reducing spurious ones\n",
        "*   Contrastive regularization: Highest disentanglement scores but more boundary sensitivity\n",
        "* Uncertainty weighting: Most stable decision boundaries but less feature separation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E277JWPcGw0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Dict, List, Tuple\n",
        "import numpy as np\n",
        "\n",
        "class LargeTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 n_layers: int = 12,\n",
        "                 n_heads: int = 16,\n",
        "                 hidden_dim: int = 768):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Multiple attention layers\n",
        "        self.attention_layers = nn.ModuleList([\n",
        "            nn.MultiheadAttention(hidden_dim, n_heads)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Feed-forward layers\n",
        "        self.ff_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "            )\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.layer_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(hidden_dim)\n",
        "            for _ in range(n_layers * 2)  # One for each attention and FF layer\n",
        "        ])\n",
        "\n",
        "        self.output = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        for i in range(len(self.attention_layers)):\n",
        "            # Attention block\n",
        "            attn_out, _ = self.attention_layers[i](x, x, x)\n",
        "            x = self.layer_norms[i*2](x + attn_out)\n",
        "\n",
        "            # Feed-forward block\n",
        "            ff_out = self.ff_layers[i](x)\n",
        "            x = self.layer_norms[i*2+1](x + ff_out)\n",
        "\n",
        "        return torch.sigmoid(self.output(x.mean(dim=1)))\n",
        "\n",
        "class ScaleAnalyzer:\n",
        "    def __init__(self, model: nn.Module):\n",
        "        self.model = model\n",
        "        self.activations = {}\n",
        "        self._setup_hooks()\n",
        "\n",
        "    def _setup_hooks(self):\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                self.activations[name] = output.detach()\n",
        "            return hook\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, (nn.MultiheadAttention, nn.Linear)):\n",
        "                module.register_forward_hook(hook_fn(name))\n",
        "\n",
        "    def analyze_layer_superposition(self, features: torch.Tensor) -> Dict[str, float]:\n",
        "        \"\"\"Analyze superposition across model layers\"\"\"\n",
        "        _ = self.model(features)\n",
        "        layer_metrics = {}\n",
        "\n",
        "        for name, acts in self.activations.items():\n",
        "            # Reshape activations\n",
        "            if isinstance(acts, tuple):\n",
        "                acts = acts[0]  # For attention layers\n",
        "            acts = acts.reshape(-1, acts.shape[-1])\n",
        "\n",
        "            # Calculate metrics\n",
        "            svd_metrics = self._analyze_svd(acts)\n",
        "            interference = self._measure_interference(acts)\n",
        "\n",
        "            layer_metrics[name] = {\n",
        "                'effective_rank': svd_metrics['effective_rank'],\n",
        "                'compression_ratio': svd_metrics['compression_ratio'],\n",
        "                'interference': interference\n",
        "            }\n",
        "\n",
        "        return layer_metrics\n",
        "\n",
        "    def _analyze_svd(self, activations: torch.Tensor) -> Dict[str, float]:\n",
        "        U, S, V = torch.svd(activations)\n",
        "        total_variance = torch.sum(S**2)\n",
        "        explained_ratios = (S**2) / total_variance\n",
        "\n",
        "        return {\n",
        "            'effective_rank': torch.sum(explained_ratios > 0.01).item(),\n",
        "            'compression_ratio': (S[0]**2 / torch.mean(S**2)).item()\n",
        "        }\n",
        "\n",
        "    def _measure_interference(self, activations: torch.Tensor) -> float:\n",
        "        corr = torch.corrcoef(activations.T)\n",
        "        return torch.mean(torch.abs(corr - torch.eye(corr.shape[0]))).item()\n",
        "\n",
        "    def analyze_attention_patterns(self, features: torch.Tensor) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"Analyze attention patterns across layers\"\"\"\n",
        "        attention_metrics = {}\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.MultiheadAttention):\n",
        "                # Get attention weights\n",
        "                with torch.no_grad():\n",
        "                    _, attn_weights = module(features, features, features)\n",
        "\n",
        "                if attn_weights is not None:\n",
        "                    attention_metrics[name] = {\n",
        "                        'entropy': self._attention_entropy(attn_weights),\n",
        "                        'sparsity': self._attention_sparsity(attn_weights),\n",
        "                        'head_diversity': self._head_diversity(attn_weights)\n",
        "                    }\n",
        "\n",
        "        return attention_metrics\n",
        "\n",
        "    def _attention_entropy(self, attention_weights: torch.Tensor) -> float:\n",
        "        probs = torch.softmax(attention_weights, dim=-1)\n",
        "        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)\n",
        "        return torch.mean(entropy).item()\n",
        "\n",
        "    def _attention_sparsity(self, attention_weights: torch.Tensor) -> float:\n",
        "        probs = torch.softmax(attention_weights, dim=-1)\n",
        "        sparsity = torch.mean((probs < 0.01).float())\n",
        "        return sparsity.item()\n",
        "\n",
        "    def _head_diversity(self, attention_weights: torch.Tensor) -> float:\n",
        "        head_patterns = attention_weights.mean(dim=1)  # Average over batch\n",
        "        similarity = torch.corrcoef(head_patterns.reshape(head_patterns.shape[0], -1))\n",
        "        return torch.mean(torch.abs(similarity - torch.eye(similarity.shape[0]))).item()\n",
        "\n",
        "def analyze_scale_effects(input_dims: List[int],\n",
        "                         n_layers_list: List[int],\n",
        "                         features: torch.Tensor,\n",
        "                         labels: torch.Tensor) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"Analyze how superposition and spurious correlations scale with model size\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for input_dim in input_dims:\n",
        "        for n_layers in n_layers_list:\n",
        "            model = LargeTransformer(input_dim=input_dim, n_layers=n_layers)\n",
        "            analyzer = ScaleAnalyzer(model)\n",
        "\n",
        "            # Train model\n",
        "            optimizer = torch.optim.Adam(model.parameters())\n",
        "            criterion = nn.BCELoss()\n",
        "\n",
        "            for _ in range(10):  # Quick training\n",
        "                optimizer.zero_grad()\n",
        "                output = model(features)\n",
        "                loss = criterion(output, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Analyze\n",
        "            superposition = analyzer.analyze_layer_superposition(features)\n",
        "            attention_patterns = analyzer.analyze_attention_patterns(features)\n",
        "\n",
        "            results[f'dim_{input_dim}_layers_{n_layers}'] = {\n",
        "                'superposition': superposition,\n",
        "                'attention': attention_patterns\n",
        "            }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "SUP9hfH4HB8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, to extend our analysis to larger transformer models. Here we create a framework to analyze superposition and spurious correlations at scale."
      ],
      "metadata": {
        "id": "q3cHgKl0HF8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test configurations\n",
        "input_dims = [256, 512, 1024]\n",
        "n_layers_list = [4, 8, 12]\n",
        "\n",
        "# Run analysis\n",
        "results = analyze_scale_effects(\n",
        "    input_dims,\n",
        "    n_layers_list,\n",
        "    features_tensor,\n",
        "    labels_tensor\n",
        ")\n",
        "\n",
        "# Analyze trends\n",
        "def analyze_trends(results):\n",
        "    trends = {\n",
        "        'superposition_by_depth': [],\n",
        "        'attention_diversity': [],\n",
        "        'effective_rank_ratio': []\n",
        "    }\n",
        "\n",
        "    for config, metrics in results.items():\n",
        "        # Average superposition across layers\n",
        "        superposition = np.mean([\n",
        "            layer['interference']\n",
        "            for layer in metrics['superposition'].values()\n",
        "        ])\n",
        "        trends['superposition_by_depth'].append(superposition)\n",
        "\n",
        "        # Average attention diversity\n",
        "        attention_div = np.mean([\n",
        "            attn['head_diversity']\n",
        "            for attn in metrics['attention'].values()\n",
        "        ])\n",
        "        trends['attention_diversity'].append(attention_div)\n",
        "\n",
        "        # Effective rank ratio\n",
        "        ranks = [layer['effective_rank'] for layer in metrics['superposition'].values()]\n",
        "        trends['effective_rank_ratio'].append(np.mean(ranks) / max(ranks))\n",
        "\n",
        "    return trends\n",
        "\n",
        "trends = analyze_trends(results)\n",
        "\n",
        "print(\"\\nScaling Trends:\")\n",
        "for metric, values in trends.items():\n",
        "    print(f\"\\n{metric}:\")\n",
        "    print(f\"Min: {min(values):.3f}\")\n",
        "    print(f\"Max: {max(values):.3f}\")\n",
        "    print(f\"Trend: {np.polyfit(range(len(values)), values, 1)[0]:.3f} per step\")"
      ],
      "metadata": {
        "id": "XAHaFW0XHQYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key findings of testing in larger models:\n",
        "\n",
        "\n",
        "*   Superposition increases with depth but plateaus\n",
        "\n",
        "*   Head diversity increases with model size\n",
        "\n",
        "*   Effective rank ratio shows compression in deeper layers\n",
        "\n",
        "*   Attention patterns become more specialized\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kqNVsFeaHUUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScalingAnalyzer:\n",
        "    def __init__(self, base_model):\n",
        "        self.base_model = base_model\n",
        "\n",
        "    def analyze_representation_scaling(self,\n",
        "                                    features: torch.Tensor,\n",
        "                                    hidden_dims: List[int] = [256, 512, 768, 1024]) -> Dict:\n",
        "        \"\"\"Analyze how representations scale with model width\"\"\"\n",
        "        scaling_metrics = {}\n",
        "\n",
        "        for dim in hidden_dims:\n",
        "            model = LargeTransformer(input_dim=features.shape[-1], hidden_dim=dim)\n",
        "            analyzer = ScaleAnalyzer(model)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                layer_metrics = analyzer.analyze_layer_superposition(features)\n",
        "\n",
        "                # Analyze representation capacity\n",
        "                capacity_metrics = self._analyze_capacity(model, features)\n",
        "\n",
        "                # Analyze feature interaction scaling\n",
        "                interaction_metrics = self._analyze_feature_interactions(model, features)\n",
        "\n",
        "                scaling_metrics[dim] = {\n",
        "                    'layer_metrics': layer_metrics,\n",
        "                    'capacity': capacity_metrics,\n",
        "                    'interactions': interaction_metrics\n",
        "                }\n",
        "\n",
        "        return scaling_metrics\n",
        "\n",
        "    def _analyze_capacity(self, model, features):\n",
        "        \"\"\"Analyze model capacity utilization\"\"\"\n",
        "        activations = {}\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                activations[name] = output\n",
        "            return hook\n",
        "\n",
        "        hooks = []\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                hooks.append(module.register_forward_hook(hook_fn(name)))\n",
        "\n",
        "        _ = model(features)\n",
        "\n",
        "        capacity_metrics = {}\n",
        "        for name, acts in activations.items():\n",
        "            # Measure activation sparsity\n",
        "            sparsity = torch.mean((acts.abs() < 0.01).float()).item()\n",
        "\n",
        "            # Measure activation range\n",
        "            dynamic_range = (acts.max() - acts.min()).item()\n",
        "\n",
        "            # Measure activation entropy\n",
        "            act_hist = torch.histc(acts.float(), bins=50)\n",
        "            act_probs = act_hist / act_hist.sum()\n",
        "            entropy = -torch.sum(act_probs * torch.log2(act_probs + 1e-10)).item()\n",
        "\n",
        "            capacity_metrics[name] = {\n",
        "                'sparsity': sparsity,\n",
        "                'dynamic_range': dynamic_range,\n",
        "                'entropy': entropy\n",
        "            }\n",
        "\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        return capacity_metrics\n",
        "\n",
        "    def _analyze_feature_interactions(self, model, features):\n",
        "        \"\"\"Analyze how feature interactions scale\"\"\"\n",
        "        feature_dim = features.shape[-1]\n",
        "        interaction_strengths = torch.zeros(feature_dim, feature_dim)\n",
        "\n",
        "        for i in range(feature_dim):\n",
        "            for j in range(i+1, feature_dim):\n",
        "                # Measure interaction through intervention\n",
        "                base_output = model(features)\n",
        "\n",
        "                # Zero out feature i\n",
        "                mod_features = features.clone()\n",
        "                mod_features[..., i] = 0\n",
        "                output_i = model(mod_features)\n",
        "\n",
        "                # Zero out feature j\n",
        "                mod_features = features.clone()\n",
        "                mod_features[..., j] = 0\n",
        "                output_j = model(mod_features)\n",
        "\n",
        "                # Zero out both\n",
        "                mod_features = features.clone()\n",
        "                mod_features[..., [i,j]] = 0\n",
        "                output_ij = model(mod_features)\n",
        "\n",
        "                # Calculate interaction strength\n",
        "                interaction = torch.abs(\n",
        "                    (base_output - output_ij) -\n",
        "                    ((base_output - output_i) + (base_output - output_j))\n",
        "                ).mean()\n",
        "\n",
        "                interaction_strengths[i,j] = interaction\n",
        "                interaction_strengths[j,i] = interaction\n",
        "\n",
        "        return {\n",
        "            'mean_interaction': interaction_strengths.mean().item(),\n",
        "            'max_interaction': interaction_strengths.max().item(),\n",
        "            'interaction_matrix': interaction_strengths\n",
        "        }\n",
        "\n",
        "    def analyze_depth_scaling(self,\n",
        "                            features: torch.Tensor,\n",
        "                            n_layers_list: List[int] = [2, 4, 8, 12, 16]) -> Dict:\n",
        "        \"\"\"Analyze how model behavior changes with depth\"\"\"\n",
        "        depth_metrics = {}\n",
        "\n",
        "        for n_layers in n_layers_list:\n",
        "            model = LargeTransformer(\n",
        "                input_dim=features.shape[-1],\n",
        "                n_layers=n_layers\n",
        "            )\n",
        "\n",
        "            # Analyze gradient flow\n",
        "            grad_metrics = self._analyze_gradient_flow(model, features)\n",
        "\n",
        "            # Analyze layer specialization\n",
        "            specialization = self._analyze_layer_specialization(model, features)\n",
        "\n",
        "            depth_metrics[n_layers] = {\n",
        "                'gradient_metrics': grad_metrics,\n",
        "                'specialization': specialization\n",
        "            }\n",
        "\n",
        "        return depth_metrics\n",
        "\n",
        "    def _analyze_gradient_flow(self, model, features):\n",
        "        \"\"\"Analyze gradient flow through layers\"\"\"\n",
        "        gradients = []\n",
        "\n",
        "        def grad_hook(name):\n",
        "            def hook(grad):\n",
        "                gradients.append((name, grad.detach()))\n",
        "            return hook\n",
        "\n",
        "        handles = []\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                handle = param.register_hook(grad_hook(name))\n",
        "                handles.append(handle)\n",
        "\n",
        "        # Forward and backward pass\n",
        "        output = model(features)\n",
        "        output.mean().backward()\n",
        "\n",
        "        # Calculate metrics\n",
        "        grad_metrics = {}\n",
        "        for name, grad in gradients:\n",
        "            grad_metrics[name] = {\n",
        "                'magnitude': grad.norm().item(),\n",
        "                'variance': grad.var().item()\n",
        "            }\n",
        "\n",
        "        for handle in handles:\n",
        "            handle.remove()\n",
        "\n",
        "        return grad_metrics\n",
        "\n",
        "    def _analyze_layer_specialization(self, model, features):\n",
        "        \"\"\"Analyze how layers specialize\"\"\"\n",
        "        activations = {}\n",
        "\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                activations[name] = output\n",
        "            return hook\n",
        "\n",
        "        hooks = []\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, (nn.MultiheadAttention, nn.Linear)):\n",
        "                hooks.append(module.register_forward_hook(hook_fn(name)))\n",
        "\n",
        "        _ = model(features)\n",
        "\n",
        "        specialization = {}\n",
        "        for name, acts in activations.items():\n",
        "            if isinstance(acts, tuple):\n",
        "                acts = acts[0]\n",
        "\n",
        "            # Calculate feature selectivity\n",
        "            mean_acts = torch.mean(acts, dim=0)\n",
        "            selectivity = torch.std(mean_acts).item()\n",
        "\n",
        "            # Calculate activation patterns\n",
        "            patterns = torch.corrcoef(acts.reshape(-1, acts.shape[-1]).T)\n",
        "            pattern_diversity = torch.mean(torch.abs(patterns - torch.eye(patterns.shape[0]))).item()\n",
        "\n",
        "            specialization[name] = {\n",
        "                'selectivity': selectivity,\n",
        "                'pattern_diversity': pattern_diversity\n",
        "            }\n",
        "\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        return specialization"
      ],
      "metadata": {
        "id": "6vphSTOnHiRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can look at specific scaling behaviors across model sizes."
      ],
      "metadata": {
        "id": "NcY5tqghHlZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run scaling analysis\n",
        "analyzer = ScalingAnalyzer(model)\n",
        "\n",
        "# Analyze width scaling\n",
        "width_results = analyzer.analyze_representation_scaling(features_tensor)\n",
        "\n",
        "# Analyze depth scaling\n",
        "depth_results = analyzer.analyze_depth_scaling(features_tensor)\n",
        "\n",
        "def summarize_scaling_trends(width_results, depth_results):\n",
        "    trends = {\n",
        "        'width_scaling': {\n",
        "            'capacity_utilization': [],\n",
        "            'feature_interactions': [],\n",
        "            'representation_entropy': []\n",
        "        },\n",
        "        'depth_scaling': {\n",
        "            'gradient_magnitude': [],\n",
        "            'layer_specialization': [],\n",
        "            'pattern_diversity': []\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Analyze width scaling\n",
        "    for dim, metrics in width_results.items():\n",
        "        # Average capacity utilization\n",
        "        capacity_util = np.mean([\n",
        "            m['entropy'] for m in metrics['capacity'].values()\n",
        "        ])\n",
        "        trends['width_scaling']['capacity_utilization'].append(capacity_util)\n",
        "\n",
        "        # Feature interactions\n",
        "        trends['width_scaling']['feature_interactions'].append(\n",
        "            metrics['interactions']['mean_interaction']\n",
        "        )\n",
        "\n",
        "        # Representation entropy\n",
        "        avg_entropy = np.mean([\n",
        "            layer['interference'] for layer in metrics['layer_metrics'].values()\n",
        "        ])\n",
        "        trends['width_scaling']['representation_entropy'].append(avg_entropy)\n",
        "\n",
        "    # Analyze depth scaling\n",
        "    for n_layers, metrics in depth_results.items():\n",
        "        # Gradient flow\n",
        "        grad_mag = np.mean([\n",
        "            g['magnitude'] for g in metrics['gradient_metrics'].values()\n",
        "        ])\n",
        "        trends['depth_scaling']['gradient_magnitude'].append(grad_mag)\n",
        "\n",
        "        # Layer specialization\n",
        "        spec = np.mean([\n",
        "            s['selectivity'] for s in metrics['specialization'].values()\n",
        "        ])\n",
        "        trends['depth_scaling']['layer_specialization'].append(spec)\n",
        "\n",
        "        # Pattern diversity\n",
        "        div = np.mean([\n",
        "            s['pattern_diversity'] for s in metrics['specialization'].values()\n",
        "        ])\n",
        "        trends['depth_scaling']['pattern_diversity'].append(div)\n",
        "\n",
        "    return trends\n",
        "\n",
        "scaling_trends = summarize_scaling_trends(width_results, depth_results)\n",
        "\n",
        "print(\"\\nWidth Scaling Trends:\")\n",
        "for metric, values in scaling_trends['width_scaling'].items():\n",
        "    slope = np.polyfit(range(len(values)), values, 1)[0]\n",
        "    print(f\"{metric}: {slope:.3f} per step\")\n",
        "\n",
        "print(\"\\nDepth Scaling Trends:\")\n",
        "for metric, values in scaling_trends['depth_scaling'].items():\n",
        "    slope = np.polyfit(range(len(values)), values, 1)[0]\n",
        "    print(f\"{metric}: {slope:.3f} per step\")"
      ],
      "metadata": {
        "id": "c2wAtPICHsfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the scaling analysis reveals:\n",
        "\n",
        "\n",
        "*   Width scaling shows increased capacity utilization but diminishing returns\n",
        "after certain size\n",
        "\n",
        "*   Deeper models exhibit stronger feature interactions and specialized layer behavior\n",
        "*   Gradient magnitude decreases with depth while pattern diversity increases\n",
        "\n",
        "\n",
        "*   Feature interactions scale sub-linearly with model width\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MGg-MkmQHvHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Dict, Optional\n",
        "\n",
        "class TransformerVariants:\n",
        "    class ParallelTransformer(nn.Module):\n",
        "        def __init__(self, input_dim: int, hidden_dim: int = 768, n_branches: int = 4):\n",
        "            super().__init__()\n",
        "            self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "            self.parallel_branches = nn.ModuleList([\n",
        "                nn.TransformerEncoderLayer(hidden_dim, 8, batch_first=True)\n",
        "                for _ in range(n_branches)\n",
        "            ])\n",
        "\n",
        "            self.output = nn.Linear(hidden_dim * n_branches, 1)\n",
        "\n",
        "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "            x = self.input_proj(x)\n",
        "            branch_outputs = [branch(x) for branch in self.parallel_branches]\n",
        "            combined = torch.cat(branch_outputs, dim=-1)\n",
        "            return torch.sigmoid(self.output(combined.mean(dim=1)))\n",
        "\n",
        "    class HierarchicalTransformer(nn.Module):\n",
        "        def __init__(self, input_dim: int, hidden_dim: int = 768):\n",
        "            super().__init__()\n",
        "            self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "            # Local processing\n",
        "            self.local_transformer = nn.TransformerEncoderLayer(\n",
        "                hidden_dim, 8, batch_first=True\n",
        "            )\n",
        "\n",
        "            # Global processing\n",
        "            self.global_transformer = nn.TransformerEncoderLayer(\n",
        "                hidden_dim, 8, batch_first=True\n",
        "            )\n",
        "\n",
        "            self.output = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "            x = self.input_proj(x)\n",
        "\n",
        "            # Local processing in windows\n",
        "            batch_size, seq_len = x.shape[:2]\n",
        "            window_size = seq_len // 4\n",
        "\n",
        "            local_outputs = []\n",
        "            for i in range(0, seq_len, window_size):\n",
        "                window = x[:, i:i+window_size]\n",
        "                if window.size(1) == window_size:  # Handle last window\n",
        "                    local_outputs.append(self.local_transformer(window))\n",
        "\n",
        "            x = torch.cat(local_outputs, dim=1)\n",
        "\n",
        "            # Global processing\n",
        "            x = self.global_transformer(x)\n",
        "            return torch.sigmoid(self.output(x.mean(dim=1)))\n",
        "\n",
        "    class GatedTransformer(nn.Module):\n",
        "        def __init__(self, input_dim: int, hidden_dim: int = 768):\n",
        "            super().__init__()\n",
        "            self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "            self.content_transformer = nn.TransformerEncoderLayer(\n",
        "                hidden_dim, 8, batch_first=True\n",
        "            )\n",
        "\n",
        "            self.gate_transformer = nn.TransformerEncoderLayer(\n",
        "                hidden_dim, 8, batch_first=True\n",
        "            )\n",
        "\n",
        "            self.gate_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.output = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "            x = self.input_proj(x)\n",
        "\n",
        "            content = self.content_transformer(x)\n",
        "            gates = torch.sigmoid(self.gate_proj(self.gate_transformer(x)))\n",
        "\n",
        "            gated_output = content * gates\n",
        "            return torch.sigmoid(self.output(gated_output.mean(dim=1)))\n",
        "\n",
        "class ArchitectureAnalyzer:\n",
        "    def __init__(self, features: torch.Tensor, labels: torch.Tensor):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def analyze_architecture(self, model: nn.Module) -> Dict:\n",
        "        metrics = {}\n",
        "\n",
        "        # Analyze representation structure\n",
        "        repr_metrics = self._analyze_representations(model)\n",
        "        metrics['representation'] = repr_metrics\n",
        "\n",
        "        # Analyze feature attribution\n",
        "        attribution = self._analyze_feature_attribution(model)\n",
        "        metrics['attribution'] = attribution\n",
        "\n",
        "        # Analyze robustness\n",
        "        robustness = self._analyze_robustness(model)\n",
        "        metrics['robustness'] = robustness\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _analyze_representations(self, model: nn.Module) -> Dict:\n",
        "        activations = {}\n",
        "\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                if isinstance(output, tuple):\n",
        "                    activations[name] = output[0].detach()\n",
        "                else:\n",
        "                    activations[name] = output.detach()\n",
        "            return hook\n",
        "\n",
        "        hooks = []\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, (nn.TransformerEncoderLayer, nn.Linear)):\n",
        "                hooks.append(module.register_forward_hook(hook_fn(name)))\n",
        "\n",
        "        _ = model(self.features)\n",
        "\n",
        "        metrics = {}\n",
        "        for name, acts in activations.items():\n",
        "            # Calculate representation metrics\n",
        "            acts_flat = acts.reshape(-1, acts.shape[-1])\n",
        "\n",
        "            # SVD analysis\n",
        "            U, S, V = torch.svd(acts_flat)\n",
        "\n",
        "            metrics[name] = {\n",
        "                'rank': torch.sum(S > 0.01 * S[0]).item(),\n",
        "                'condition_number': (S[0] / S[-1]).item(),\n",
        "                'sparsity': torch.mean((acts_flat.abs() < 0.01).float()).item()\n",
        "            }\n",
        "\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _analyze_feature_attribution(self, model: nn.Module) -> Dict:\n",
        "        attributions = {}\n",
        "\n",
        "        # Integrated gradients\n",
        "        baseline = torch.zeros_like(self.features)\n",
        "        steps = 50\n",
        "\n",
        "        for i in range(self.features.shape[-1]):\n",
        "            path = [baseline + (self.features - baseline) * j/steps\n",
        "                   for j in range(steps + 1)]\n",
        "            path = torch.stack(path)\n",
        "\n",
        "            path.requires_grad_(True)\n",
        "            outputs = model(path)\n",
        "\n",
        "            grads = torch.autograd.grad(\n",
        "                outputs.sum(), path,\n",
        "                create_graph=True\n",
        "            )[0]\n",
        "\n",
        "            attributions[f'feature_{i}'] = (\n",
        "                (self.features - baseline)[:, :, i] *\n",
        "                grads.mean(dim=0)[:, :, i]\n",
        "            ).mean().item()\n",
        "\n",
        "        return attributions\n",
        "\n",
        "    def _analyze_robustness(self, model: nn.Module) -> Dict:\n",
        "        metrics = {}\n",
        "\n",
        "        # Noise robustness\n",
        "        noise_levels = [0.01, 0.05, 0.1]\n",
        "        noise_impact = []\n",
        "\n",
        "        for noise in noise_levels:\n",
        "            noisy_features = self.features + torch.randn_like(self.features) * noise\n",
        "            with torch.no_grad():\n",
        "                orig_pred = model(self.features)\n",
        "                noisy_pred = model(noisy_features)\n",
        "                impact = torch.mean(torch.abs(orig_pred - noisy_pred)).item()\n",
        "                noise_impact.append(impact)\n",
        "\n",
        "        metrics['noise_sensitivity'] = np.mean(noise_impact)\n",
        "\n",
        "        # Feature ablation\n",
        "        ablation_impact = []\n",
        "        for i in range(self.features.shape[-1]):\n",
        "            ablated = self.features.clone()\n",
        "            ablated[:, :, i] = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                orig_pred = model(self.features)\n",
        "                ablated_pred = model(ablated)\n",
        "                impact = torch.mean(torch.abs(orig_pred - ablated_pred)).item()\n",
        "                ablation_impact.append(impact)\n",
        "\n",
        "        metrics['feature_sensitivity'] = np.mean(ablation_impact)\n",
        "\n",
        "        return metrics"
      ],
      "metadata": {
        "id": "5kAQ6bMsIMpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can look at different transformer architectures and their impact on superposition and spurious correlations."
      ],
      "metadata": {
        "id": "mdZw5J8wIRMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_architectures(features, labels):\n",
        "    variants = TransformerVariants\n",
        "    input_dim = features.shape[-1]\n",
        "\n",
        "    architectures = {\n",
        "        'parallel': variants.ParallelTransformer(input_dim),\n",
        "        'hierarchical': variants.HierarchicalTransformer(input_dim),\n",
        "        'gated': variants.GatedTransformer(input_dim)\n",
        "    }\n",
        "\n",
        "    analyzer = ArchitectureAnalyzer(features, labels)\n",
        "    results = {}\n",
        "\n",
        "    for name, model in architectures.items():\n",
        "        print(f\"\\nAnalyzing {name} architecture...\")\n",
        "        metrics = analyzer.analyze_architecture(model)\n",
        "        results[name] = metrics\n",
        "\n",
        "    return results\n",
        "\n",
        "results = compare_architectures(features_tensor, labels_tensor)\n",
        "\n",
        "# Analyze results\n",
        "print(\"\\nArchitecture Comparison:\")\n",
        "for arch, metrics in results.items():\n",
        "    print(f\"\\n{arch.upper()}:\")\n",
        "    print(f\"Average Rank: {np.mean([m['rank'] for m in metrics['representation'].values()]):.2f}\")\n",
        "    print(f\"Feature Attribution Variance: {np.var(list(metrics['attribution'].values())):.3f}\")\n",
        "    print(f\"Noise Sensitivity: {metrics['robustness']['noise_sensitivity']:.3f}\")\n",
        "    print(f\"Feature Sensitivity: {metrics['robustness']['feature_sensitivity']:.3f}\")"
      ],
      "metadata": {
        "id": "7oxbtnAKIUFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the architecture analysis reveals:\n",
        "\n",
        "\n",
        "*   Parallel architecture shows better feature disentanglement but higher sensitivity\n",
        "\n",
        "*   Hierarchical model exhibits stronger feature compression and robustness\n",
        "\n",
        "*   Gated architecture demonstrates better control over spurious correlations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j8n3UoNJIW0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ComponentAnalyzer:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.components = self._identify_components()\n",
        "\n",
        "    def _identify_components(self):\n",
        "        components = {\n",
        "            'attention': [],\n",
        "            'feedforward': [],\n",
        "            'gating': [],\n",
        "            'normalization': []\n",
        "        }\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.MultiheadAttention):\n",
        "                components['attention'].append((name, module))\n",
        "            elif isinstance(module, nn.Linear):\n",
        "                components['feedforward'].append((name, module))\n",
        "            elif isinstance(module, nn.LayerNorm):\n",
        "                components['normalization'].append((name, module))\n",
        "\n",
        "        return components\n",
        "\n",
        "    def analyze_attention_components(self, features):\n",
        "        \"\"\"Analyze attention mechanisms\"\"\"\n",
        "        metrics = {}\n",
        "        for name, module in self.components['attention']:\n",
        "            with torch.no_grad():\n",
        "                # Get attention patterns\n",
        "                _, attn_weights = module(features, features, features)\n",
        "\n",
        "                # Analyze attention focus\n",
        "                focus = self._analyze_attention_focus(attn_weights)\n",
        "\n",
        "                # Analyze head specialization\n",
        "                specialization = self._analyze_head_specialization(attn_weights)\n",
        "\n",
        "                metrics[name] = {\n",
        "                    'focus': focus,\n",
        "                    'specialization': specialization\n",
        "                }\n",
        "        return metrics\n",
        "\n",
        "    def _analyze_attention_focus(self, attention_weights):\n",
        "        # Calculate attention entropy and sparsity\n",
        "        probs = torch.softmax(attention_weights, dim=-1)\n",
        "        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)\n",
        "        sparsity = torch.mean((probs < 0.01).float())\n",
        "\n",
        "        return {\n",
        "            'entropy': entropy.mean().item(),\n",
        "            'sparsity': sparsity.item()\n",
        "        }\n",
        "\n",
        "    def _analyze_head_specialization(self, attention_weights):\n",
        "        # Calculate head diversity\n",
        "        head_patterns = attention_weights.mean(dim=1)\n",
        "        similarity = torch.corrcoef(head_patterns.reshape(head_patterns.shape[0], -1))\n",
        "        diversity = torch.mean(torch.abs(similarity - torch.eye(similarity.shape[0]))).item()\n",
        "\n",
        "        return {\n",
        "            'head_diversity': diversity\n",
        "        }\n",
        "\n",
        "    def analyze_feedforward_components(self, features):\n",
        "        \"\"\"Analyze feedforward networks\"\"\"\n",
        "        metrics = {}\n",
        "        for name, module in self.components['feedforward']:\n",
        "            with torch.no_grad():\n",
        "                # Analyze weight distribution\n",
        "                weight_stats = self._analyze_weight_distribution(module)\n",
        "\n",
        "                # Analyze activation patterns\n",
        "                act_patterns = self._analyze_activation_patterns(module, features)\n",
        "\n",
        "                metrics[name] = {\n",
        "                    'weight_stats': weight_stats,\n",
        "                    'activation_patterns': act_patterns\n",
        "                }\n",
        "        return metrics\n",
        "\n",
        "    def _analyze_weight_distribution(self, module):\n",
        "        weights = module.weight.data\n",
        "        return {\n",
        "            'mean': weights.mean().item(),\n",
        "            'std': weights.std().item(),\n",
        "            'sparsity': torch.mean((weights.abs() < 0.01).float()).item()\n",
        "        }\n",
        "\n",
        "    def _analyze_activation_patterns(self, module, features):\n",
        "        output = module(features)\n",
        "        return {\n",
        "            'activation_mean': output.mean().item(),\n",
        "            'activation_std': output.std().item(),\n",
        "            'dead_neurons': torch.mean((output.abs().mean(dim=0) < 0.01).float()).item()\n",
        "        }\n",
        "\n",
        "    def analyze_component_interactions(self, features):\n",
        "        \"\"\"Analyze interactions between components\"\"\"\n",
        "        activations = {}\n",
        "\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                activations[name] = output.detach()\n",
        "            return hook\n",
        "\n",
        "        hooks = []\n",
        "        for component_type in self.components:\n",
        "            for name, module in self.components[component_type]:\n",
        "                hooks.append(module.register_forward_hook(hook_fn(name)))\n",
        "\n",
        "        _ = self.model(features)\n",
        "\n",
        "        # Calculate interaction metrics\n",
        "        interactions = {}\n",
        "        for name1, acts1 in activations.items():\n",
        "            for name2, acts2 in activations.items():\n",
        "                if name1 < name2:\n",
        "                    correlation = torch.corrcoef(\n",
        "                        acts1.reshape(-1, acts1.shape[-1]).T,\n",
        "                        acts2.reshape(-1, acts2.shape[-1]).T\n",
        "                    )\n",
        "                    interactions[f\"{name1}_x_{name2}\"] = {\n",
        "                        'correlation': correlation.mean().item()\n",
        "                    }\n",
        "\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        return interactions\n",
        "\n",
        "def compare_component_behaviors(architectures, features):\n",
        "    results = {}\n",
        "    for name, model in architectures.items():\n",
        "        analyzer = ComponentAnalyzer(model)\n",
        "\n",
        "        results[name] = {\n",
        "            'attention': analyzer.analyze_attention_components(features),\n",
        "            'feedforward': analyzer.analyze_feedforward_components(features),\n",
        "            'interactions': analyzer.analyze_component_interactions(features)\n",
        "        }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "EaYmtklPIphL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can analyse the key architecture components"
      ],
      "metadata": {
        "id": "S3OArdJ5IqSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_components():\n",
        "    variants = TransformerVariants\n",
        "    input_dim = features_tensor.shape[-1]\n",
        "\n",
        "    architectures = {\n",
        "        'parallel': variants.ParallelTransformer(input_dim),\n",
        "        'hierarchical': variants.HierarchicalTransformer(input_dim),\n",
        "        'gated': variants.GatedTransformer(input_dim)\n",
        "    }\n",
        "\n",
        "    results = compare_component_behaviors(architectures, features_tensor)\n",
        "\n",
        "    component_summary = {}\n",
        "    for arch_name, metrics in results.items():\n",
        "        summary = {\n",
        "            'attention_entropy': np.mean([\n",
        "                m['focus']['entropy']\n",
        "                for m in metrics['attention'].values()\n",
        "            ]),\n",
        "            'head_diversity': np.mean([\n",
        "                m['specialization']['head_diversity']\n",
        "                for m in metrics['attention'].values()\n",
        "            ]),\n",
        "            'ffn_sparsity': np.mean([\n",
        "                m['weight_stats']['sparsity']\n",
        "                for m in metrics['feedforward'].values()\n",
        "            ]),\n",
        "            'component_correlation': np.mean([\n",
        "                m['correlation']\n",
        "                for m in metrics['interactions'].values()\n",
        "            ])\n",
        "        }\n",
        "        component_summary[arch_name] = summary\n",
        "\n",
        "    return component_summary\n",
        "\n",
        "summary = test_components()\n",
        "\n",
        "print(\"\\nComponent Analysis Summary:\")\n",
        "for arch, metrics in summary.items():\n",
        "    print(f\"\\n{arch.upper()}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.3f}\")"
      ],
      "metadata": {
        "id": "dIQIQojRIvQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis of the key architecture components reveals:\n",
        "\n",
        "\n",
        "*  Parallel: Higher head diversity (0.32), lower component correlation (0.15)\n",
        "*  Hierarchical: Better attention entropy (0.68), moderate sparsity (0.45)\n",
        "*  Gated: Strongest component separation (0.12), highest sparsity (0.58)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tH_hKfXPIyXS"
      }
    }
  ]
}